<!DOCTYPE html>

<html lang="en" data-content_root="../">
  <head>
    <meta charset="utf-8" />
    <meta name="viewport" content="width=device-width, initial-scale=1.0" /><meta name="viewport" content="width=device-width, initial-scale=1" />
<meta property="og:title" content="Introduction to Probabilitic Graphical Models" />
<meta property="og:type" content="website" />
<meta property="og:url" content="detailed_notebooks/1.%20Introduction%20to%20Probabilistic%20Graphical%20Models.html" />
<meta property="og:site_name" content="pgmpy" />
<meta property="og:description" content="Contents: What is machine learning, Different ways of learning from data, Why probabilistic graphical models, Major types of PGMs. 1. What is machine learning: Machine learning is a scientific disc..." />
<meta property="og:image:width" content="1146" />
<meta property="og:image:height" content="600" />
<meta property="og:image" content="_images/social_previews/summary_detailed_notebooks_1. Introduction to Probabilistic Graphical Models_60913096.png" />
<meta property="og:image:alt" content="Contents: What is machine learning, Different ways of learning from data, Why probabilistic graphical models, Major types of PGMs. 1. What is machine learning: Machine learning is a scientific disc..." />
<meta name="description" content="Contents: What is machine learning, Different ways of learning from data, Why probabilistic graphical models, Major types of PGMs. 1. What is machine learning: Machine learning is a scientific disc..." />
<meta name="twitter:card" content="summary_large_image" />

    <title>Introduction to Probabilitic Graphical Models &#8212; 1.0.0 | pgmpy docs</title>
    <link rel="stylesheet" type="text/css" href="../_static/pygments.css?v=fa44fd50" />
    <link rel="stylesheet" type="text/css" href="../_static/alabaster.css?v=7b53859b" />
    <link rel="stylesheet" type="text/css" href="../_static/copybutton.css?v=76b2166b" />
    <link rel="stylesheet" type="text/css" href="../_static/sphinx-design.min.css?v=95c83b7e" />
    <link rel="stylesheet" type="text/css" href="../_static/nbsphinx-code-cells.css?v=2aa19091" />
    <script src="../_static/documentation_options.js?v=8d563738"></script>
    <script src="../_static/doctools.js?v=9bcbadda"></script>
    <script src="../_static/sphinx_highlight.js?v=dc90522c"></script>
    <script src="../_static/clipboard.min.js?v=a7894cd8"></script>
    <script src="../_static/copybutton.js?v=f281be69"></script>
    <script src="../_static/design-tabs.js?v=f930bc37"></script>
    <script crossorigin="anonymous" integrity="sha256-Ae2Vz/4ePdIu6ZyI/5ZGsYnb+m0JlOmKPjt6XZ9JJkA=" src="https://cdnjs.cloudflare.com/ajax/libs/require.js/2.3.4/require.min.js"></script>
    <link rel="canonical" href="https://pgmpy.org/detailed_notebooks/1.%20Introduction%20to%20Probabilistic%20Graphical%20Models.html" />
    <link rel="icon" href="../_static/logo_favi.ico"/>
    <link rel="index" title="Index" href="../genindex.html" />
    <link rel="search" title="Search" href="../search.html" />
    <link rel="next" title="Bayesian Network" href="2.%20Bayesian%20Networks.html" />
    <link rel="prev" title="Tutorial Notebooks" href="../tutorial.html" />
   
  <link rel="stylesheet" href="../_static/custom.css" type="text/css" />
  

  
  

  </head><body>
  

    <div class="document">
      <div class="documentwrapper">
        <div class="bodywrapper">
          

          <div class="body" role="main">
            
  <section id="Introduction-to-Probabilitic-Graphical-Models">
<h1>Introduction to Probabilitic Graphical Models<a class="headerlink" href="#Introduction-to-Probabilitic-Graphical-Models" title="Link to this heading">¶</a></h1>
<div class="nbinput nblast docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[1]:
</pre></div>
</div>
<div class="input_area highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span> <span class="nn">IPython.display</span> <span class="kn">import</span> <span class="n">Image</span>
</pre></div>
</div>
</div>
<section id="Contents">
<h2>Contents<a class="headerlink" href="#Contents" title="Link to this heading">¶</a></h2>
<ol class="arabic simple">
<li><p>What is machine learning</p></li>
<li><p>Different ways of learning from data</p></li>
<li><p>Why probabilistic graphical models</p></li>
<li><p>Major types of PGMs</p></li>
</ol>
<section id="1.-What-is-machine-learning">
<h3>1. What is machine learning<a class="headerlink" href="#1.-What-is-machine-learning" title="Link to this heading">¶</a></h3>
<p>Machine learning is a scientific discipline that explores the construction and study of algorithms that can learn from data. Such algorithms operate by building a model from example inputs and using that to make predictions or decisions, rather than following strictly static program instructions.</p>
<p>We can take an example of predicting the type of flower based on the sepal length and width of the flower. Let’s say we have some data (discretized iris data set on sepal length and width). The dataset looks something like this:</p>
<div class="nbinput docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[2]:
</pre></div>
</div>
<div class="input_area highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="o">%</span><span class="k">run</span> ../scripts/1/discretize.py
<span class="n">data</span>
</pre></div>
</div>
</div>
<div class="nboutput nblast docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[2]:
</pre></div>
</div>
<div class="output_area rendered_html docutils container">
<div>
<style scoped>
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }

    .dataframe tbody tr th {
        vertical-align: top;
    }

    .dataframe thead th {
        text-align: right;
    }
</style>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>length</th>
      <th>width</th>
      <th>type</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>134</th>
      <td>6</td>
      <td>3</td>
      <td>2</td>
    </tr>
    <tr>
      <th>148</th>
      <td>6</td>
      <td>3</td>
      <td>2</td>
    </tr>
    <tr>
      <th>25</th>
      <td>5</td>
      <td>3</td>
      <td>0</td>
    </tr>
    <tr>
      <th>48</th>
      <td>5</td>
      <td>4</td>
      <td>0</td>
    </tr>
    <tr>
      <th>86</th>
      <td>7</td>
      <td>3</td>
      <td>1</td>
    </tr>
    <tr>
      <th>...</th>
      <td>...</td>
      <td>...</td>
      <td>...</td>
    </tr>
    <tr>
      <th>79</th>
      <td>6</td>
      <td>3</td>
      <td>1</td>
    </tr>
    <tr>
      <th>26</th>
      <td>5</td>
      <td>3</td>
      <td>0</td>
    </tr>
    <tr>
      <th>101</th>
      <td>6</td>
      <td>3</td>
      <td>2</td>
    </tr>
    <tr>
      <th>71</th>
      <td>6</td>
      <td>3</td>
      <td>1</td>
    </tr>
    <tr>
      <th>119</th>
      <td>6</td>
      <td>2</td>
      <td>2</td>
    </tr>
  </tbody>
</table>
<p>150 rows × 3 columns</p>
</div></div>
</div>
</section>
<section id="2.-Different-ways-of-learning-from-data">
<h3>2. Different ways of learning from data<a class="headerlink" href="#2.-Different-ways-of-learning-from-data" title="Link to this heading">¶</a></h3>
<p>Now let’s say we want to predict the type of flower for a new given data point. There are multiple ways to solve this problem. We will consider these two ways in some detail:</p>
<ol class="arabic simple">
<li><p>We could find a function which can directly map an input value to it’s class label.</p></li>
<li><p>We can find the probability distributions over the variables and then use this distribution to answer queries about the new data point.</p></li>
</ol>
<p>There are a lot of algorithms for finding a mapping function. For example linear regression tries to find a linear equation which explains the data. Support vector machine tries to find a plane which separates the data points. Decision Tree tries to find a set of simple greater than and less than equations to classify the data. Let’s try to apply Decision Tree on this data set.</p>
<p>We can plot the data and it looks something like this:</p>
<div class="nbinput docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[3]:
</pre></div>
</div>
<div class="input_area highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="o">%</span><span class="k">matplotlib</span> inline
<span class="kn">import</span> <span class="nn">matplotlib.pyplot</span> <span class="k">as</span> <span class="nn">plt</span>
<span class="kn">import</span> <span class="nn">numpy</span> <span class="k">as</span> <span class="nn">np</span>

<span class="c1"># Adding a little bit of noise so that it&#39;s easier to visualize</span>
<span class="n">data_with_noise</span> <span class="o">=</span> <span class="n">data</span><span class="o">.</span><span class="n">iloc</span><span class="p">[:,</span> <span class="p">:</span><span class="mi">2</span><span class="p">]</span> <span class="o">+</span> <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">normal</span><span class="p">(</span><span class="n">loc</span><span class="o">=</span><span class="mi">0</span><span class="p">,</span> <span class="n">scale</span><span class="o">=</span><span class="mf">0.1</span><span class="p">,</span> <span class="n">size</span><span class="o">=</span><span class="p">(</span><span class="mi">150</span><span class="p">,</span> <span class="mi">2</span><span class="p">))</span>
<span class="n">plt</span><span class="o">.</span><span class="n">scatter</span><span class="p">(</span><span class="n">data_with_noise</span><span class="o">.</span><span class="n">length</span><span class="p">,</span> <span class="n">data_with_noise</span><span class="o">.</span><span class="n">width</span><span class="p">,</span> <span class="n">c</span><span class="o">=</span><span class="p">[</span> <span class="s2">&quot;bgr&quot;</span><span class="p">[</span><span class="n">k</span><span class="p">]</span> <span class="k">for</span> <span class="n">k</span> <span class="ow">in</span> <span class="n">data</span><span class="o">.</span><span class="n">iloc</span><span class="p">[:,</span><span class="mi">2</span><span class="p">]</span> <span class="p">],</span> <span class="n">s</span><span class="o">=</span><span class="mi">200</span><span class="p">,</span> <span class="n">alpha</span><span class="o">=</span><span class="mf">0.3</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="nboutput docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[3]:
</pre></div>
</div>
<div class="output_area docutils container">
<div class="highlight"><pre>
&lt;matplotlib.collections.PathCollection at 0x134b10890&gt;
</pre></div></div>
</div>
<div class="nboutput nblast docutils container">
<div class="prompt empty docutils container">
</div>
<div class="output_area docutils container">
<img alt="../../../pgmpy_docs/doctrees/nbsphinx/detailed_notebooks_1._Introduction_to_Probabilistic_Graphical_Models_6_1.png" src="../../../pgmpy_docs/doctrees/nbsphinx/detailed_notebooks_1._Introduction_to_Probabilistic_Graphical_Models_6_1.png" />
</div>
</div>
<p>In the plot we can easily see that the blue points are concentrated on the top-left corner, green ones in bottom left and red ones in top right.</p>
<p>Now let’s try to train a Decision Tree on this data.</p>
<div class="nbinput docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[4]:
</pre></div>
</div>
<div class="input_area highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span> <span class="nn">sklearn.tree</span> <span class="kn">import</span> <span class="n">DecisionTreeClassifier</span>
<span class="kn">from</span> <span class="nn">sklearn.model_selection</span> <span class="kn">import</span> <span class="n">train_test_split</span>

<span class="n">X_train</span><span class="p">,</span> <span class="n">X_test</span><span class="p">,</span> <span class="n">y_train</span><span class="p">,</span> <span class="n">y_test</span> <span class="o">=</span> <span class="n">train_test_split</span><span class="p">(</span><span class="n">data</span><span class="p">[[</span><span class="s1">&#39;length&#39;</span><span class="p">,</span> <span class="s1">&#39;width&#39;</span><span class="p">]]</span><span class="o">.</span><span class="n">values</span><span class="p">,</span> <span class="n">data</span><span class="o">.</span><span class="n">type</span><span class="o">.</span><span class="n">values</span><span class="p">,</span> <span class="n">test_size</span><span class="o">=</span><span class="mf">0.2</span><span class="p">)</span>

<span class="n">classifier</span> <span class="o">=</span> <span class="n">DecisionTreeClassifier</span><span class="p">(</span><span class="n">max_depth</span><span class="o">=</span><span class="mi">4</span><span class="p">)</span>
<span class="n">classifier</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">X_train</span><span class="p">,</span> <span class="n">y_train</span><span class="p">)</span>
<span class="n">classifier</span><span class="o">.</span><span class="n">predict</span><span class="p">(</span><span class="n">X_test</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="nboutput nblast docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[4]:
</pre></div>
</div>
<div class="output_area docutils container">
<div class="highlight"><pre>
array([0, 2, 0, 2, 2, 1, 2, 0, 0, 2, 2, 2, 2, 2, 0, 2, 0, 1, 2, 2, 2, 2,
       2, 2, 2, 2, 2, 0, 0, 2])
</pre></div></div>
</div>
<div class="nbinput docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[5]:
</pre></div>
</div>
<div class="input_area highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">classifier</span><span class="o">.</span><span class="n">score</span><span class="p">(</span><span class="n">X_test</span><span class="p">,</span> <span class="n">y_test</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="nboutput nblast docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[5]:
</pre></div>
</div>
<div class="output_area docutils container">
<div class="highlight"><pre>
0.5666666666666667
</pre></div></div>
</div>
<p>So, in this case we got a classification accuracy of 60 %.</p>
<p>Now moving on to our second approach using a probabilistic model. The most obvious way to do this classification task would be to compute a Joint Probability Distribution over all these variables and then marginalize and reduce over these according to our new data point to get the probabilities of classes.</p>
<div class="nbinput nblast docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[6]:
</pre></div>
</div>
<div class="input_area highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">X_train</span><span class="p">,</span> <span class="n">X_test</span> <span class="o">=</span> <span class="n">data</span><span class="p">[:</span><span class="mi">120</span><span class="p">],</span> <span class="n">data</span><span class="p">[</span><span class="mi">120</span><span class="p">:]</span>
</pre></div>
</div>
</div>
<div class="nbinput docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[7]:
</pre></div>
</div>
<div class="input_area highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">X_train</span>
</pre></div>
</div>
</div>
<div class="nboutput nblast docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[7]:
</pre></div>
</div>
<div class="output_area rendered_html docutils container">
<div>
<style scoped>
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }

    .dataframe tbody tr th {
        vertical-align: top;
    }

    .dataframe thead th {
        text-align: right;
    }
</style>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>length</th>
      <th>width</th>
      <th>type</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>134</th>
      <td>6</td>
      <td>3</td>
      <td>2</td>
    </tr>
    <tr>
      <th>148</th>
      <td>6</td>
      <td>3</td>
      <td>2</td>
    </tr>
    <tr>
      <th>25</th>
      <td>5</td>
      <td>3</td>
      <td>0</td>
    </tr>
    <tr>
      <th>48</th>
      <td>5</td>
      <td>4</td>
      <td>0</td>
    </tr>
    <tr>
      <th>86</th>
      <td>7</td>
      <td>3</td>
      <td>1</td>
    </tr>
    <tr>
      <th>...</th>
      <td>...</td>
      <td>...</td>
      <td>...</td>
    </tr>
    <tr>
      <th>135</th>
      <td>8</td>
      <td>3</td>
      <td>2</td>
    </tr>
    <tr>
      <th>6</th>
      <td>5</td>
      <td>3</td>
      <td>0</td>
    </tr>
    <tr>
      <th>69</th>
      <td>6</td>
      <td>2</td>
      <td>1</td>
    </tr>
    <tr>
      <th>139</th>
      <td>7</td>
      <td>3</td>
      <td>2</td>
    </tr>
    <tr>
      <th>90</th>
      <td>6</td>
      <td>3</td>
      <td>1</td>
    </tr>
  </tbody>
</table>
<p>120 rows × 3 columns</p>
</div></div>
</div>
<div class="nbinput docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[8]:
</pre></div>
</div>
<div class="input_area highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1"># Computing the joint probability distribution over the training data</span>
<span class="n">joint_prob</span> <span class="o">=</span> <span class="n">X_train</span><span class="o">.</span><span class="n">groupby</span><span class="p">([</span><span class="s1">&#39;length&#39;</span><span class="p">,</span> <span class="s1">&#39;width&#39;</span><span class="p">,</span> <span class="s1">&#39;type&#39;</span><span class="p">])</span><span class="o">.</span><span class="n">size</span><span class="p">()</span> <span class="o">/</span> <span class="mi">120</span>
<span class="n">joint_prob</span>
</pre></div>
</div>
</div>
<div class="nboutput nblast docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[8]:
</pre></div>
</div>
<div class="output_area docutils container">
<div class="highlight"><pre>
length  width  type
4       2      0       0.008333
        3      0       0.033333
5       2      1       0.033333
               2       0.008333
        3      0       0.191667
               1       0.016667
        4      0       0.141667
6       2      1       0.075000
               2       0.025000
        3      1       0.225000
               2       0.200000
        4      0       0.041667
7       2      2       0.008333
        3      1       0.066667
               2       0.116667
        4      2       0.008333
8       3      2       0.033333
        4      2       0.016667
dtype: float64
</pre></div></div>
</div>
<div class="nbinput docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[9]:
</pre></div>
</div>
<div class="input_area highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1"># Predicting values</span>

<span class="c1"># Selecting just the feature variables.</span>
<span class="n">X_test_features</span> <span class="o">=</span> <span class="n">X_test</span><span class="o">.</span><span class="n">iloc</span><span class="p">[:,</span> <span class="p">:</span><span class="mi">2</span><span class="p">]</span><span class="o">.</span><span class="n">values</span>
<span class="n">X_test_actual_results</span> <span class="o">=</span> <span class="n">X_test</span><span class="o">.</span><span class="n">iloc</span><span class="p">[:,</span> <span class="mi">2</span><span class="p">]</span><span class="o">.</span><span class="n">values</span>

<span class="n">predicted_values</span> <span class="o">=</span> <span class="p">[]</span>
<span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="n">X_test_features</span><span class="p">:</span>
    <span class="n">predicted_values</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">joint_prob</span><span class="p">[</span><span class="n">i</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="n">i</span><span class="p">[</span><span class="mi">1</span><span class="p">]]</span><span class="o">.</span><span class="n">idxmax</span><span class="p">())</span>

<span class="n">predicted_values</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">(</span><span class="n">predicted_values</span><span class="p">)</span>
<span class="n">predicted_values</span>
</pre></div>
</div>
</div>
<div class="nboutput nblast docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[9]:
</pre></div>
</div>
<div class="output_area docutils container">
<div class="highlight"><pre>
array([1, 1, 0, 2, 1, 1, 0, 0, 1, 1, 1, 1, 0, 1, 1, 0, 2, 2, 1, 1, 1, 0,
       1, 1, 1, 1, 0, 1, 1, 1])
</pre></div></div>
</div>
<div class="nbinput docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[10]:
</pre></div>
</div>
<div class="input_area highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1"># Comparing results with the actual data.</span>
<span class="n">predicted_values</span> <span class="o">==</span> <span class="n">X_test_actual_results</span>
</pre></div>
</div>
</div>
<div class="nboutput nblast docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[10]:
</pre></div>
</div>
<div class="output_area docutils container">
<div class="highlight"><pre>
array([ True, False,  True,  True,  True,  True,  True,  True,  True,
       False, False,  True,  True, False,  True,  True, False, False,
       False,  True,  True,  True,  True, False, False,  True,  True,
       False,  True, False])
</pre></div></div>
</div>
<div class="nbinput docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[11]:
</pre></div>
</div>
<div class="input_area highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">score</span> <span class="o">=</span> <span class="p">(</span><span class="n">predicted_values</span> <span class="o">==</span> <span class="n">X_test_actual_results</span><span class="p">)</span><span class="o">.</span><span class="n">sum</span><span class="p">()</span> <span class="o">/</span> <span class="mi">30</span>
<span class="nb">print</span><span class="p">(</span><span class="n">score</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="nboutput nblast docutils container">
<div class="prompt empty docutils container">
</div>
<div class="output_area docutils container">
<div class="highlight"><pre>
0.6333333333333333
</pre></div></div>
</div>
</section>
<section id="Why-Probabilistic-Graphical-Models">
<h3>Why Probabilistic Graphical Models<a class="headerlink" href="#Why-Probabilistic-Graphical-Models" title="Link to this heading">¶</a></h3>
<p>In the previous example we saw how Bayesian Inference works. We construct a Joint Distribution over the data and then condition on the observed variable to compute the posterior distribution. And then we query on this posterior distribution to predict the values of new data points.</p>
<p>But the problem with this method is that the Joint Probability Distribution is exponential to the number of states (cardinality) of each variable. So, for problems having a lot of features or having high cardinality of features, inference becomes a difficult task because of computational limitations. For example, for 10 random variables each having 10 states, the size of the Joint Distribution would be 10^10.</p>
<p><strong>Proababilistic Graphical Models (PGM)</strong>: PGM is a technique of compactly representing Joint Probability Distribution over random variables by exploiting the (conditional) independencies between the variables. PGM also provides us methods for efficiently doing inference over these joint distributions.</p>
<p>Each graphical model is characterized by a graph structure (can be directed, undirected or both) and a set of parameters associated with each graph.</p>
<p>The problem in the above example can be represented using a Bayesian Model (a type of graphical model) as:</p>
<div class="nbinput docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[12]:
</pre></div>
</div>
<div class="input_area highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">Image</span><span class="p">(</span><span class="n">filename</span><span class="o">=</span><span class="s1">&#39;../images/1/Iris_BN.png&#39;</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="nboutput nblast docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[12]:
</pre></div>
</div>
<div class="output_area docutils container">
<img alt="../../../pgmpy_docs/doctrees/nbsphinx/detailed_notebooks_1._Introduction_to_Probabilistic_Graphical_Models_20_0.png" src="../../../pgmpy_docs/doctrees/nbsphinx/detailed_notebooks_1._Introduction_to_Probabilistic_Graphical_Models_20_0.png" />
</div>
</div>
<p>In this case the parameters of the network would be <img class="math" src="../_images/math/08c8073736dcb9f8ce2cefe56896f4e9f75665d5.png" alt="P(L)"/>, <img class="math" src="../_images/math/1d629a21deb80159ab43e8f94ba7cedb750c99fb.png" alt="P(W)"/> and <img class="math" src="../_images/math/e4e9649b7f59388a099a7c01fa40b0ad203fb016.png" alt="P(T | L, W)"/>. So, we will need to store 5 values for <img class="math" src="../_images/math/19eef1966f7c545af3ac8c0fa486974d873e3c65.png" alt="L"/>, 3 values for <img class="math" src="../_images/math/1fbee781f84569077719a167b64e12064360fac1.png" alt="W"/> and 45 values for <img class="math" src="../_images/math/e4e9649b7f59388a099a7c01fa40b0ad203fb016.png" alt="P(T | L, W)"/>. So, a total of 45 + 5 + 3 = 53 values to completely parameterize the network which is actually more than 45 values which we need for <img class="math" src="../_images/math/f9cf9ff4f2a6a579335532532fed652615118aa9.png" alt="P (T, L, W)"/>. But in the cases of bigger networks graphical models help in saving space. We can take the example of the student network shown
below:</p>
<div class="nbinput docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[13]:
</pre></div>
</div>
<div class="input_area highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">Image</span><span class="p">(</span><span class="n">filename</span><span class="o">=</span><span class="s1">&#39;../images/1/student.png&#39;</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="nboutput nblast docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[13]:
</pre></div>
</div>
<div class="output_area docutils container">
<img alt="../../../pgmpy_docs/doctrees/nbsphinx/detailed_notebooks_1._Introduction_to_Probabilistic_Graphical_Models_22_0.png" src="../../../pgmpy_docs/doctrees/nbsphinx/detailed_notebooks_1._Introduction_to_Probabilistic_Graphical_Models_22_0.png" />
</div>
</div>
<p>Considering that <img class="math" src="../_images/math/0fcab9067b50b87e868c4fd70f213a086addb964.png" alt="D"/> has cardinality of 2, <img class="math" src="../_images/math/015755a22b6219b345c36a9a47b091dc56007486.png" alt="I"/> has cardinality of 2, <img class="math" src="../_images/math/b988975be41fd13b4d091c10202ba19374643586.png" alt="S"/> has cardinality of 2, <img class="math" src="../_images/math/89878909dbb648acdc4a44ded1bd982d7bddef5d.png" alt="G"/> has cardinality of 3 and <img class="math" src="../_images/math/19eef1966f7c545af3ac8c0fa486974d873e3c65.png" alt="L"/> has cardinality of 2. Also the parameters in this network would be <img class="math" src="../_images/math/5b5adfe8a95b80443a3e62df8f3819820a9d44d5.png" alt="P(D)"/>, <img class="math" src="../_images/math/9eeb06c4c447658203a8201f0597940702bf5ab9.png" alt="P(I)"/>, <img class="math" src="../_images/math/42ff0fb4017ef55875341a744664e48ff8b9fe0a.png" alt="P(S | I)"/>, <img class="math" src="../_images/math/1ecb093009be08d2981f834ffdc2e3164d02ef9f.png" alt="P(G | D, I)"/>, <img class="math" src="../_images/math/9160475a94084068403bdcfed9e43c539c13c6c3.png" alt="P(L | G)"/>. So, the number of values needed would be 2 for <img class="math" src="../_images/math/5b5adfe8a95b80443a3e62df8f3819820a9d44d5.png" alt="P(D)"/>, 2 for <img class="math" src="../_images/math/9eeb06c4c447658203a8201f0597940702bf5ab9.png" alt="P(I)"/>, 12 for <img class="math" src="../_images/math/1ecb093009be08d2981f834ffdc2e3164d02ef9f.png" alt="P(G | D, I)"/>, 6 for <img class="math" src="../_images/math/9160475a94084068403bdcfed9e43c539c13c6c3.png" alt="P(L | G)"/>, 4 for <img class="math" src="../_images/math/42ff0fb4017ef55875341a744664e48ff8b9fe0a.png" alt="P(S | I)"/>, total of 4 + 6 + 12 + 2 + 2 = 26
compared to 2 * 2 * 3 * 2 * 2 = 48 required for the Joint Distribution over all the variables.</p>
<section id="Types-of-Graphical-Models">
<h4>Types of Graphical Models<a class="headerlink" href="#Types-of-Graphical-Models" title="Link to this heading">¶</a></h4>
<p>There are mainly 2 types of graphical models:</p>
<ol class="arabic simple">
<li><p>Bayesian Models: A Bayesian Model consists of a directed graph and Conditional Probability Distributions(CPDs) associated with each of the node. Each CPD is of the form <img class="math" src="../_images/math/c73399ed4cb8d2cad96156bb49392b5991a99822.png" alt="P(node | parents(node))"/> where <img class="math" src="../_images/math/3730fc0dd02cd5bdf06a83c4dd5be69cd023d796.png" alt="parents(node)"/> are the parents of the node in the graph structure.</p></li>
<li><p>Markov Models: A Markov Models consists of an undirected graph and are parameterized by Factors. Factors represent how much 2 or more variables agree with each other.</p></li>
</ol>
</section>
</section>
</section>
</section>


          </div>
          
        </div>
      </div>
      <div class="sphinxsidebar" role="navigation" aria-label="Main">
        <div class="sphinxsidebarwrapper">
<p class="logo">
  <a href="../index.html">
    <img class="logo" src="../_static/logo.png" alt="Logo" />
    
  </a>
</p>









<search id="searchbox" style="display: none" role="search">
  <h3 id="searchlabel">Quick search</h3>
    <div class="searchformwrapper">
    <form class="search" action="../search.html" method="get">
      <input type="text" name="q" aria-labelledby="searchlabel" autocomplete="off" autocorrect="off" autocapitalize="off" spellcheck="false"/>
      <input type="submit" value="Go" />
    </form>
    </div>
</search>
<script>document.getElementById('searchbox').style.display = "block"</script><h3>Navigation</h3>
<ul class="current">
<li class="toctree-l1"><a class="reference internal" href="../started/base.html">Getting Started</a></li>
<li class="toctree-l1"><a class="reference internal" href="../examples.html">Examples</a></li>
<li class="toctree-l1"><a class="reference internal" href="../models/base.html">Supported Models</a></li>
<li class="toctree-l1"><a class="reference internal" href="../factors/base.html">Parameterization</a></li>
<li class="toctree-l1"><a class="reference internal" href="../infer/base.html">Probabilistic Inference</a></li>
<li class="toctree-l1"><a class="reference internal" href="../causal_infer/base.html">Causal Inference</a></li>
<li class="toctree-l1"><a class="reference internal" href="../param_estimator/base.html">Parameter Estimation</a></li>
<li class="toctree-l1"><a class="reference internal" href="../structure_estimator/base.html">Causal Discovery / Structure Learning</a></li>
<li class="toctree-l1"><a class="reference internal" href="../metrics/metrics.html">Metrics for Testing Models</a></li>
<li class="toctree-l1"><a class="reference internal" href="../readwrite/base.html">Reading/Writing to File</a></li>
<li class="toctree-l1"><a class="reference internal" href="../plotting.html">Plotting Models</a></li>
<li class="toctree-l1 current"><a class="reference internal" href="../tutorial.html">Tutorial Notebooks</a><ul class="current">
<li class="toctree-l2 current"><a class="current reference internal" href="#">Introduction to Probabilitic Graphical Models</a></li>
<li class="toctree-l2"><a class="reference internal" href="2.%20Bayesian%20Networks.html">Bayesian Network</a></li>
<li class="toctree-l2"><a class="reference internal" href="3.%20Causal%20Bayesian%20Networks.html">Causal Bayesian Networks</a></li>
<li class="toctree-l2"><a class="reference internal" href="4.%20Markov%20Models.html">Markov Networks</a></li>
<li class="toctree-l2"><a class="reference internal" href="5.%20Exact%20Inference%20in%20Graphical%20Models.html">Exact Inference in Graphical Models</a></li>
<li class="toctree-l2"><a class="reference internal" href="6.%20Approximate%20Inference%20in%20Graphical%20Models.html">Approximate Inference in Graphical Models</a></li>
<li class="toctree-l2"><a class="reference internal" href="7.%20Parameterizing%20with%20Continuous%20Variables.html">Parameterizing with Continuous Variables</a></li>
<li class="toctree-l2"><a class="reference internal" href="8.%20Sampling%20Algorithms.html">Sampling In Continuous Graphical Models</a></li>
<li class="toctree-l2"><a class="reference internal" href="9.%20Reading%20and%20Writing%20from%20pgmpy%20file%20formats.html">Reading and Writing from pgmpy file formats</a></li>
<li class="toctree-l2"><a class="reference internal" href="10.%20Learning%20Bayesian%20Networks%20from%20Data.html">Learning Bayesian Networks from Data</a></li>
<li class="toctree-l2"><a class="reference internal" href="11.%20A%20Bayesian%20Network%20to%20model%20the%20influence%20of%20energy%20consumption%20on%20greenhouse%20gases%20in%20Italy.html">A Bayesian Network to model the influence of energy consumption on greenhouse gases in Italy</a></li>
</ul>
</li>
</ul>

<div class="relations">
<h3>Related Topics</h3>
<ul>
  <li><a href="../index.html">Documentation overview</a><ul>
  <li><a href="../tutorial.html">Tutorial Notebooks</a><ul>
      <li>Previous: <a href="../tutorial.html" title="previous chapter">Tutorial Notebooks</a></li>
      <li>Next: <a href="2.%20Bayesian%20Networks.html" title="next chapter">Bayesian Network</a></li>
  </ul></li>
  </ul></li>
</ul>
</div><script async src="https://media.ethicalads.io/media/client/ethicalads.min.js"></script>

<div data-ea-publisher="pgmpyorg" data-ea-type="image" data-ea-style="horizontal"></div><script async src="https://www.googletagmanager.com/gtag/js?id=G-HCFR07M31W"></script>
<script>
  window.dataLayer = window.dataLayer || [];
  function gtag(){dataLayer.push(arguments);}
  gtag('js', new Date());

  gtag('config', 'G-HCFR07M31W');
</script>
        </div>
      </div>
      <div class="clearer"></div>
    </div>
    <div class="footer">
      &#169;2025, Ankur Ankan.
      
      |
      Powered by <a href="https://www.sphinx-doc.org/">Sphinx 8.2.3</a>
      &amp; <a href="https://alabaster.readthedocs.io">Alabaster 0.7.16</a>
      
      |
      <a href="../_sources/detailed_notebooks/1. Introduction to Probabilistic Graphical Models.ipynb.txt"
          rel="nofollow">Page source</a>
    </div>

    

    
  </body>
</html>
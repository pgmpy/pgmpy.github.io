<!DOCTYPE html>

<html lang="en" data-content_root="../">
  <head>
    <meta charset="utf-8" />
    <meta name="viewport" content="width=device-width, initial-scale=1.0" /><meta name="viewport" content="width=device-width, initial-scale=1" />
<meta property="og:title" content="Hill Climb Search" />
<meta property="og:type" content="website" />
<meta property="og:url" content="structure_estimator/hill.html" />
<meta property="og:site_name" content="pgmpy" />
<meta property="og:description" content="Structure Scores: BDeu Score: BIC Score: AIC Score: K2 Score: BDs Score: Gaussian Log-Likelihood Score: Gaussian BIC Score: Gaussian AIC Score: Conditional Gaussian Log-Likelihood Score: Conditiona..." />
<meta property="og:image:width" content="1146" />
<meta property="og:image:height" content="600" />
<meta property="og:image" content="_images/social_previews/summary_structure_estimator_hill_c3685b93.png" />
<meta property="og:image:alt" content="Structure Scores: BDeu Score: BIC Score: AIC Score: K2 Score: BDs Score: Gaussian Log-Likelihood Score: Gaussian BIC Score: Gaussian AIC Score: Conditional Gaussian Log-Likelihood Score: Conditiona..." />
<meta name="description" content="Structure Scores: BDeu Score: BIC Score: AIC Score: K2 Score: BDs Score: Gaussian Log-Likelihood Score: Gaussian BIC Score: Gaussian AIC Score: Conditional Gaussian Log-Likelihood Score: Conditiona..." />
<meta name="twitter:card" content="summary_large_image" />

    <title>Hill Climb Search &#8212; 1.0.0 | pgmpy docs</title>
    <link rel="stylesheet" type="text/css" href="../_static/pygments.css?v=fa44fd50" />
    <link rel="stylesheet" type="text/css" href="../_static/alabaster.css?v=7b53859b" />
    <link rel="stylesheet" type="text/css" href="../_static/copybutton.css?v=76b2166b" />
    <link rel="stylesheet" type="text/css" href="../_static/sphinx-design.min.css?v=95c83b7e" />
    <script src="../_static/documentation_options.js?v=8d563738"></script>
    <script src="../_static/doctools.js?v=9bcbadda"></script>
    <script src="../_static/sphinx_highlight.js?v=dc90522c"></script>
    <script src="../_static/clipboard.min.js?v=a7894cd8"></script>
    <script src="../_static/copybutton.js?v=f281be69"></script>
    <script src="../_static/design-tabs.js?v=f930bc37"></script>
    <script crossorigin="anonymous" integrity="sha256-Ae2Vz/4ePdIu6ZyI/5ZGsYnb+m0JlOmKPjt6XZ9JJkA=" src="https://cdnjs.cloudflare.com/ajax/libs/require.js/2.3.4/require.min.js"></script>
    <link rel="canonical" href="https://pgmpy.org/structure_estimator/hill.html" />
    <link rel="icon" href="../_static/logo_favi.ico"/>
    <link rel="index" title="Index" href="../genindex.html" />
    <link rel="search" title="Search" href="../search.html" />
    <link rel="next" title="Greedy Equivalence Search (GES)" href="ges.html" />
    <link rel="prev" title="PC (Constraint-Based Estimator)" href="pc.html" />
   
  <link rel="stylesheet" href="../_static/custom.css" type="text/css" />
  

  
  

  </head><body>
  

    <div class="document">
      <div class="documentwrapper">
        <div class="bodywrapper">
          

          <div class="body" role="main">
            
  <section id="hill-climb-search">
<h1>Hill Climb Search<a class="headerlink" href="#hill-climb-search" title="Link to this heading">¶</a></h1>
<dl class="py class">
<dt class="sig sig-object py" id="pgmpy.estimators.HillClimbSearch">
<em class="property"><span class="k"><span class="pre">class</span></span><span class="w"> </span></em><span class="sig-prename descclassname"><span class="pre">pgmpy.estimators.</span></span><span class="sig-name descname"><span class="pre">HillClimbSearch</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">data</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">DataFrame</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">use_cache</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">bool</span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">True</span></span></em>, <em class="sig-param"><span class="o"><span class="pre">**</span></span><span class="n"><span class="pre">kwargs</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/pgmpy/estimators/HillClimbSearch.html#HillClimbSearch"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#pgmpy.estimators.HillClimbSearch" title="Link to this definition">¶</a></dt>
<dd><p>Class for heuristic hill climb searches for DAGs, to learn
network structure from data. <cite>estimate</cite> attempts to find a model with optimal score.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters<span class="colon">:</span></dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>data</strong> (<em>pandas DataFrame object</em>) – dataframe object where each column represents one variable.
(If some values in the data are missing the data cells should be set to <cite>numpy.nan</cite>.
Note that pandas converts each column containing <cite>numpy.nan`s to dtype `float</cite>.)</p></li>
<li><p><strong>state_names</strong> (<em>dict</em><em> (</em><em>optional</em><em>)</em>) – A dict indicating, for each variable, the discrete set of states (or values)
that the variable can take. If unspecified, the observed values in the data set
are taken to be the only possible states.</p></li>
<li><p><strong>use_caching</strong> (<em>boolean</em>) – If True, uses caching of score for faster computation.
Note: Caching only works for scoring methods which are decomposable. Can
give wrong results in case of custom scoring methods.</p></li>
</ul>
</dd>
</dl>
<p class="rubric">References</p>
<p>Koller &amp; Friedman, Probabilistic Graphical Models - Principles and Techniques, 2009
Section 18.4.3 (page 811ff)</p>
<dl class="py method">
<dt class="sig sig-object py" id="pgmpy.estimators.HillClimbSearch.estimate">
<span class="sig-name descname"><span class="pre">estimate</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">scoring_method</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">str</span><span class="w"> </span><span class="p"><span class="pre">|</span></span><span class="w"> </span><span class="pre">StructureScore</span><span class="w"> </span><span class="p"><span class="pre">|</span></span><span class="w"> </span><span class="pre">None</span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">start_dag</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><a class="reference internal" href="../base.html#pgmpy.base.DAG.DAG" title="pgmpy.base.DAG.DAG"><span class="pre">DAG</span></a><span class="w"> </span><span class="p"><span class="pre">|</span></span><span class="w"> </span><span class="pre">None</span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">tabu_length</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">int</span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">100</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">max_indegree</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">int</span><span class="w"> </span><span class="p"><span class="pre">|</span></span><span class="w"> </span><span class="pre">None</span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">expert_knowledge</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">ExpertKnowledge</span><span class="w"> </span><span class="p"><span class="pre">|</span></span><span class="w"> </span><span class="pre">None</span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">epsilon</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">float</span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">0.0001</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">max_iter</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">int</span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">1000000</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">show_progress</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">bool</span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">True</span></span></em><span class="sig-paren">)</span> <span class="sig-return"><span class="sig-return-icon">&#x2192;</span> <span class="sig-return-typehint"><a class="reference internal" href="../base.html#pgmpy.base.DAG.DAG" title="pgmpy.base.DAG.DAG"><span class="pre">DAG</span></a></span></span><a class="reference internal" href="../_modules/pgmpy/estimators/HillClimbSearch.html#HillClimbSearch.estimate"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#pgmpy.estimators.HillClimbSearch.estimate" title="Link to this definition">¶</a></dt>
<dd><p>Performs local hill climb search to estimates the <cite>DAG</cite> structure that
has optimal score, according to the scoring method supplied. Starts at
model <cite>start_dag</cite> and proceeds by step-by-step network modifications
until a local maximum is reached. Only estimates network structure, no
parametrization.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters<span class="colon">:</span></dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>scoring_method</strong> (<em>str</em><em> or </em><em>StructureScore instance</em>) – The score to be optimized during structure estimation.  Supported
structure scores: k2, bdeu, bds, bic-d, aic-d, ll-g, aic-g, bic-g,
ll-cg, aic-cg, bic-cg. Also accepts a custom score, but it should
be an instance of <cite>StructureScore</cite>.</p></li>
<li><p><strong>start_dag</strong> (<em>DAG instance</em>) – The starting point for the local search. By default, a completely
disconnected network is used.</p></li>
<li><p><strong>tabu_length</strong> (<em>int</em>) – If provided, the last <cite>tabu_length</cite> graph modifications cannot be
reversed during the search procedure. This serves to enforce a
wider exploration of the search space. Default value: 100.</p></li>
<li><p><strong>max_indegree</strong> (<em>int</em><em> or </em><em>None</em>) – If provided and unequal None, the procedure only searches among models
where all nodes have at most <cite>max_indegree</cite> parents. Defaults to None.</p></li>
<li><p><strong>expert_knowledge</strong> (<em>pgmpy.estimators.ExpertKnowledge instance</em><em> (</em><em>default: None</em><em>)</em>) – Expert knowledge to be used with the algorithm. Expert knowledge
allows specification of required and forbidden edges, as well as temporal
order of nodes.</p></li>
<li><p><strong>epsilon</strong> (<em>float</em><em> (</em><em>default: 1e-4</em><em>)</em>) – Defines the exit condition. If the improvement in score is less
than <cite>epsilon</cite>, the learned model is returned.</p></li>
<li><p><strong>max_iter</strong> (<em>int</em><em> (</em><em>default: 1e6</em><em>)</em>) – The maximum number of iterations allowed. Returns the learned model
when the number of iterations is greater than <cite>max_iter</cite>.</p></li>
</ul>
</dd>
<dt class="field-even">Returns<span class="colon">:</span></dt>
<dd class="field-even"><p><strong>Estimated model</strong> – A <cite>DAG</cite> at a (local) score maximum.</p>
</dd>
<dt class="field-odd">Return type<span class="colon">:</span></dt>
<dd class="field-odd"><p><a class="reference internal" href="../models/dag.html#pgmpy.base.DAG" title="pgmpy.base.DAG">pgmpy.base.DAG</a></p>
</dd>
</dl>
<p class="rubric">Examples</p>
<div class="doctest highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="c1"># Simulate some sample data from a known model to learn the model structure from</span>
<span class="gp">&gt;&gt;&gt; </span><span class="kn">from</span> <span class="nn">pgmpy.utils</span> <span class="kn">import</span> <span class="n">get_example_model</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">model</span> <span class="o">=</span> <span class="n">get_example_model</span><span class="p">(</span><span class="s2">&quot;alarm&quot;</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">df</span> <span class="o">=</span> <span class="n">model</span><span class="o">.</span><span class="n">simulate</span><span class="p">(</span><span class="nb">int</span><span class="p">(</span><span class="mf">1e3</span><span class="p">))</span>
</pre></div>
</div>
<div class="doctest highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="c1"># Learn the model structure using HillClimbSearch algorithm from `df`</span>
<span class="gp">&gt;&gt;&gt; </span><span class="kn">from</span> <span class="nn">pgmpy.estimators</span> <span class="kn">import</span> <span class="n">HillClimbSearch</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">est</span> <span class="o">=</span> <span class="n">HillClimbSearch</span><span class="p">(</span><span class="n">data</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">dag</span> <span class="o">=</span> <span class="n">est</span><span class="o">.</span><span class="n">estimate</span><span class="p">(</span><span class="n">scoring_method</span><span class="o">=</span><span class="s2">&quot;bic-d&quot;</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="nb">len</span><span class="p">(</span><span class="n">dag</span><span class="o">.</span><span class="n">nodes</span><span class="p">())</span>
<span class="go">37</span>
<span class="gp">&gt;&gt;&gt; </span><span class="nb">len</span><span class="p">(</span><span class="n">dag</span><span class="o">.</span><span class="n">edges</span><span class="p">())</span>
<span class="go">45</span>
</pre></div>
</div>
</dd></dl>

</dd></dl>

</section>
<section id="structure-scores">
<h1>Structure Scores<a class="headerlink" href="#structure-scores" title="Link to this heading">¶</a></h1>
<section id="bdeu-score">
<h2>BDeu Score<a class="headerlink" href="#bdeu-score" title="Link to this heading">¶</a></h2>
<dl class="py class">
<dt class="sig sig-object py" id="pgmpy.estimators.BDeu">
<em class="property"><span class="k"><span class="pre">class</span></span><span class="w"> </span></em><span class="sig-prename descclassname"><span class="pre">pgmpy.estimators.</span></span><span class="sig-name descname"><span class="pre">BDeu</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">data</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">equivalent_sample_size</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">10</span></span></em>, <em class="sig-param"><span class="o"><span class="pre">**</span></span><span class="n"><span class="pre">kwargs</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/pgmpy/estimators/StructureScore.html#BDeu"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#pgmpy.estimators.BDeu" title="Link to this definition">¶</a></dt>
<dd><p>BDeu structure score for discrete Bayesian networks with Dirichlet priors.</p>
<p>The BDeu score evaluates Bayesian network structures using an “equivalent sample size”
to define Dirichlet prior hyperparameters, making it flexible for various data sizes
and uncertainty levels. Use this score when you want to control the influence of your prior
belief through the equivalent sample size.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters<span class="colon">:</span></dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>data</strong> (<em>pandas.DataFrame</em>) – DataFrame where each column represents a discrete variable.
Missing values should be set as <cite>numpy.nan</cite>.
Note: pandas converts such columns to dtype float.</p></li>
<li><p><strong>equivalent_sample_size</strong> (<em>int</em><em>, </em><em>optional</em><em> (</em><em>default: 10</em><em>)</em>) – The equivalent (imaginary) sample size for the Dirichlet hyperparameters.
The score is sensitive to this value; experiment with different values as needed.</p></li>
<li><p><strong>state_names</strong> (<em>dict</em><em>, </em><em>optional</em>) – Dictionary mapping variable names to their discrete states.
If not specified, unique values observed in the data are used as possible states.</p></li>
</ul>
</dd>
</dl>
<p class="rubric">Examples</p>
<div class="doctest highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="kn">import</span> <span class="nn">pandas</span> <span class="k">as</span> <span class="nn">pd</span>
<span class="gp">&gt;&gt;&gt; </span><span class="kn">import</span> <span class="nn">numpy</span> <span class="k">as</span> <span class="nn">np</span>
<span class="gp">&gt;&gt;&gt; </span><span class="kn">from</span> <span class="nn">pgmpy.models</span> <span class="kn">import</span> <span class="n">DiscreteBayesianNetwork</span>
<span class="gp">&gt;&gt;&gt; </span><span class="kn">from</span> <span class="nn">pgmpy.estimators</span> <span class="kn">import</span> <span class="n">BDeu</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">data</span> <span class="o">=</span> <span class="n">pd</span><span class="o">.</span><span class="n">DataFrame</span><span class="p">({</span><span class="s2">&quot;A&quot;</span><span class="p">:</span> <span class="p">[</span><span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">0</span><span class="p">],</span> <span class="s2">&quot;B&quot;</span><span class="p">:</span> <span class="p">[</span><span class="mi">1</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">0</span><span class="p">],</span> <span class="s2">&quot;C&quot;</span><span class="p">:</span> <span class="p">[</span><span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">0</span><span class="p">]})</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">model</span> <span class="o">=</span> <span class="n">DiscreteBayesianNetwork</span><span class="p">([(</span><span class="s2">&quot;A&quot;</span><span class="p">,</span> <span class="s2">&quot;B&quot;</span><span class="p">),</span> <span class="p">(</span><span class="s2">&quot;A&quot;</span><span class="p">,</span> <span class="s2">&quot;C&quot;</span><span class="p">)])</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">bdeu_score</span> <span class="o">=</span> <span class="n">BDeu</span><span class="p">(</span><span class="n">data</span><span class="p">,</span> <span class="n">equivalent_sample_size</span><span class="o">=</span><span class="mi">5</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="nb">print</span><span class="p">(</span><span class="n">bdeu_score</span><span class="o">.</span><span class="n">score</span><span class="p">(</span><span class="n">model</span><span class="p">))</span>
<span class="go">-241.872</span>
</pre></div>
</div>
<dl class="field-list simple">
<dt class="field-odd">Raises<span class="colon">:</span></dt>
<dd class="field-odd"><p><strong>ValueError</strong> – If the data contains continuous variables, or if the model variables are not present in the data.</p>
</dd>
</dl>
<p class="rubric">References</p>
<dl class="simple">
<dt>[1] Koller &amp; Friedman, Probabilistic Graphical Models - Principles and Techniques, 2009,</dt><dd><p>Section 18.3.4–18.3.6 (esp. page 806).</p>
</dd>
<dt>[2] AM Carvalho, Scoring functions for learning Bayesian networks,</dt><dd><p><a class="reference external" href="http://www.lx.it.pt/~asmc/pub/talks/09-TA/ta_pres.pdf">http://www.lx.it.pt/~asmc/pub/talks/09-TA/ta_pres.pdf</a></p>
</dd>
</dl>
<dl class="py method">
<dt class="sig sig-object py" id="pgmpy.estimators.BDeu.local_score">
<span class="sig-name descname"><span class="pre">local_score</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">variable</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">parents</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/pgmpy/estimators/StructureScore.html#BDeu.local_score"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#pgmpy.estimators.BDeu.local_score" title="Link to this definition">¶</a></dt>
<dd><p>Computes the local BDeu score for a given variable and its parent variables.</p>
<p>This method calculates how well a given variable is explained by its parents
according to the BDeu scoring metric, incorporating the equivalent sample size
as the Dirichlet prior.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters<span class="colon">:</span></dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>variable</strong> (<em>str</em>) – The name of the variable for which the local score is to be computed.</p></li>
<li><p><strong>parents</strong> (<em>list</em><em> of </em><em>str</em>) – List of variable names considered as parents of <cite>variable</cite>.</p></li>
</ul>
</dd>
<dt class="field-even">Returns<span class="colon">:</span></dt>
<dd class="field-even"><p><strong>score</strong> – The local BDeu score for the specified variable and parent configuration.</p>
</dd>
<dt class="field-odd">Return type<span class="colon">:</span></dt>
<dd class="field-odd"><p>float</p>
</dd>
<dt class="field-even">Raises<span class="colon">:</span></dt>
<dd class="field-even"><p><strong>ValueError</strong> – If <cite>variable</cite> or any parent is not found in state_names or data.</p>
</dd>
</dl>
</dd></dl>

</dd></dl>

</section>
<section id="bic-score">
<h2>BIC Score<a class="headerlink" href="#bic-score" title="Link to this heading">¶</a></h2>
<dl class="py class">
<dt class="sig sig-object py" id="pgmpy.estimators.BIC">
<em class="property"><span class="k"><span class="pre">class</span></span><span class="w"> </span></em><span class="sig-prename descclassname"><span class="pre">pgmpy.estimators.</span></span><span class="sig-name descname"><span class="pre">BIC</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">data</span></span></em>, <em class="sig-param"><span class="o"><span class="pre">**</span></span><span class="n"><span class="pre">kwargs</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/pgmpy/estimators/StructureScore.html#BIC"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#pgmpy.estimators.BIC" title="Link to this definition">¶</a></dt>
<dd><p>BIC (Bayesian Information Criterion) structure score for discrete Bayesian networks.</p>
<p>The BIC score, also known as the Minimal Descriptive Length (MDL) score, evaluates
Bayesian network structures using a log-likelihood term with a complexity penalty to
discourage overfitting. Use this score for structure learning when you want to balance
model fit with simplicity.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters<span class="colon">:</span></dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>data</strong> (<em>pandas.DataFrame</em>) – DataFrame where each column represents a discrete variable.
Missing values should be set as <cite>numpy.nan</cite>.
Note: pandas converts such columns to dtype float.</p></li>
<li><p><strong>state_names</strong> (<em>dict</em><em>, </em><em>optional</em>) – Dictionary mapping variable names to their discrete states.
If not specified, unique values observed in the data are used as possible states.</p></li>
</ul>
</dd>
</dl>
<p class="rubric">Examples</p>
<div class="doctest highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="kn">import</span> <span class="nn">pandas</span> <span class="k">as</span> <span class="nn">pd</span>
<span class="gp">&gt;&gt;&gt; </span><span class="kn">import</span> <span class="nn">numpy</span> <span class="k">as</span> <span class="nn">np</span>
<span class="gp">&gt;&gt;&gt; </span><span class="kn">from</span> <span class="nn">pgmpy.models</span> <span class="kn">import</span> <span class="n">DiscreteBayesianNetwork</span>
<span class="gp">&gt;&gt;&gt; </span><span class="kn">from</span> <span class="nn">pgmpy.estimators</span> <span class="kn">import</span> <span class="n">BIC</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">data</span> <span class="o">=</span> <span class="n">pd</span><span class="o">.</span><span class="n">DataFrame</span><span class="p">({</span><span class="s2">&quot;A&quot;</span><span class="p">:</span> <span class="p">[</span><span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">0</span><span class="p">],</span> <span class="s2">&quot;B&quot;</span><span class="p">:</span> <span class="p">[</span><span class="mi">1</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">0</span><span class="p">],</span> <span class="s2">&quot;C&quot;</span><span class="p">:</span> <span class="p">[</span><span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">0</span><span class="p">]})</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">model</span> <span class="o">=</span> <span class="n">DiscreteBayesianNetwork</span><span class="p">([(</span><span class="s2">&quot;A&quot;</span><span class="p">,</span> <span class="s2">&quot;B&quot;</span><span class="p">),</span> <span class="p">(</span><span class="s2">&quot;A&quot;</span><span class="p">,</span> <span class="s2">&quot;C&quot;</span><span class="p">)])</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">bic_score</span> <span class="o">=</span> <span class="n">BIC</span><span class="p">(</span><span class="n">data</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="nb">print</span><span class="p">(</span><span class="n">bic_score</span><span class="o">.</span><span class="n">score</span><span class="p">(</span><span class="n">model</span><span class="p">))</span>
<span class="go">-151.47</span>
</pre></div>
</div>
<dl class="field-list simple">
<dt class="field-odd">Raises<span class="colon">:</span></dt>
<dd class="field-odd"><p><strong>ValueError</strong> – If the data contains continuous variables, or if the model variables are not present in the data.</p>
</dd>
</dl>
<p class="rubric">References</p>
<dl class="simple">
<dt>[1] Koller &amp; Friedman, Probabilistic Graphical Models - Principles and Techniques, 2009,</dt><dd><p>Section 18.3.4–18.3.6 (esp. page 802).</p>
</dd>
<dt>[2] AM Carvalho, Scoring functions for learning Bayesian networks,</dt><dd><p><a class="reference external" href="http://www.lx.it.pt/~asmc/pub/talks/09-TA/ta_pres.pdf">http://www.lx.it.pt/~asmc/pub/talks/09-TA/ta_pres.pdf</a></p>
</dd>
</dl>
<dl class="py method">
<dt class="sig sig-object py" id="pgmpy.estimators.BIC.local_score">
<span class="sig-name descname"><span class="pre">local_score</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">variable</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">parents</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/pgmpy/estimators/StructureScore.html#BIC.local_score"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#pgmpy.estimators.BIC.local_score" title="Link to this definition">¶</a></dt>
<dd><p>Computes the local BIC/MDL score for a variable and its parent variables.</p>
<p>This method quantifies the fit of a variable to its parent set in the network,
balancing log-likelihood with a complexity penalty to discourage overfitting.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters<span class="colon">:</span></dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>variable</strong> (<em>str</em>) – The name of the variable (node) for which the local score is to be computed.</p></li>
<li><p><strong>parents</strong> (<em>list</em><em> of </em><em>str</em>) – List of variable names considered as parents of <cite>variable</cite>.</p></li>
</ul>
</dd>
<dt class="field-even">Returns<span class="colon">:</span></dt>
<dd class="field-even"><p><strong>score</strong> – The local BIC score for the specified variable and parent configuration.</p>
</dd>
<dt class="field-odd">Return type<span class="colon">:</span></dt>
<dd class="field-odd"><p>float</p>
</dd>
</dl>
<p class="rubric">Examples</p>
<div class="doctest highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">variable</span> <span class="o">=</span> <span class="s2">&quot;B&quot;</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">parents</span> <span class="o">=</span> <span class="p">[</span><span class="s2">&quot;A&quot;</span><span class="p">]</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">score</span> <span class="o">=</span> <span class="n">bic_score</span><span class="o">.</span><span class="n">local_score</span><span class="p">(</span><span class="n">variable</span><span class="p">,</span> <span class="n">parents</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="nb">print</span><span class="p">(</span><span class="n">score</span><span class="p">)</span>
<span class="go">-19.315</span>
</pre></div>
</div>
<dl class="field-list simple">
<dt class="field-odd">Raises<span class="colon">:</span></dt>
<dd class="field-odd"><p><strong>ValueError</strong> – If <cite>variable</cite> or any parent is not present in <cite>state_names</cite> or data, or if
    the data contains unsupported types (e.g., continuous values).</p>
</dd>
</dl>
</dd></dl>

</dd></dl>

</section>
<section id="aic-score">
<h2>AIC Score<a class="headerlink" href="#aic-score" title="Link to this heading">¶</a></h2>
<dl class="py class">
<dt class="sig sig-object py" id="pgmpy.estimators.AIC">
<em class="property"><span class="k"><span class="pre">class</span></span><span class="w"> </span></em><span class="sig-prename descclassname"><span class="pre">pgmpy.estimators.</span></span><span class="sig-name descname"><span class="pre">AIC</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">data</span></span></em>, <em class="sig-param"><span class="o"><span class="pre">**</span></span><span class="n"><span class="pre">kwargs</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/pgmpy/estimators/StructureScore.html#AIC"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#pgmpy.estimators.AIC" title="Link to this definition">¶</a></dt>
<dd><p>AIC (Akaike Information Criterion) structure score for discrete Bayesian networks.</p>
<p>The AIC score evaluates Bayesian network structures using a log-likelihood term
with a penalty for model complexity to discourage overfitting. Unlike BIC,
the penalty term is independent of sample size, making AIC more sensitive to
goodness of fit in smaller datasets.</p>
<p>Use this score when you want to select a network structure that balances model
fit with simplicity, especially in contexts with moderate or small sample sizes.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters<span class="colon">:</span></dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>data</strong> (<em>pandas.DataFrame</em>) – DataFrame where each column represents a discrete variable.
Missing values should be set as <cite>numpy.nan</cite>.
Note: pandas converts such columns to dtype float.</p></li>
<li><p><strong>state_names</strong> (<em>dict</em><em>, </em><em>optional</em>) – Dictionary mapping variable names to their discrete states.
If not specified, unique values observed in the data are used as possible states.</p></li>
</ul>
</dd>
</dl>
<p class="rubric">Examples</p>
<div class="doctest highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="kn">import</span> <span class="nn">pandas</span> <span class="k">as</span> <span class="nn">pd</span>
<span class="gp">&gt;&gt;&gt; </span><span class="kn">import</span> <span class="nn">numpy</span> <span class="k">as</span> <span class="nn">np</span>
<span class="gp">&gt;&gt;&gt; </span><span class="kn">from</span> <span class="nn">pgmpy.models</span> <span class="kn">import</span> <span class="n">DiscreteBayesianNetwork</span>
<span class="gp">&gt;&gt;&gt; </span><span class="kn">from</span> <span class="nn">pgmpy.estimators</span> <span class="kn">import</span> <span class="n">AIC</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">data</span> <span class="o">=</span> <span class="n">pd</span><span class="o">.</span><span class="n">DataFrame</span><span class="p">({</span><span class="s2">&quot;A&quot;</span><span class="p">:</span> <span class="p">[</span><span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">0</span><span class="p">],</span> <span class="s2">&quot;B&quot;</span><span class="p">:</span> <span class="p">[</span><span class="mi">1</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">0</span><span class="p">],</span> <span class="s2">&quot;C&quot;</span><span class="p">:</span> <span class="p">[</span><span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">0</span><span class="p">]})</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">model</span> <span class="o">=</span> <span class="n">DiscreteBayesianNetwork</span><span class="p">([(</span><span class="s2">&quot;A&quot;</span><span class="p">,</span> <span class="s2">&quot;B&quot;</span><span class="p">),</span> <span class="p">(</span><span class="s2">&quot;A&quot;</span><span class="p">,</span> <span class="s2">&quot;C&quot;</span><span class="p">)])</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">aic_score</span> <span class="o">=</span> <span class="n">AIC</span><span class="p">(</span><span class="n">data</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="nb">print</span><span class="p">(</span><span class="n">aic_score</span><span class="o">.</span><span class="n">score</span><span class="p">(</span><span class="n">model</span><span class="p">))</span>
<span class="go">-140.12</span>
</pre></div>
</div>
<dl class="field-list simple">
<dt class="field-odd">Raises<span class="colon">:</span></dt>
<dd class="field-odd"><p><strong>ValueError</strong> – If the data contains continuous variables, or if the model variables are not present in the data.</p>
</dd>
</dl>
<p class="rubric">References</p>
<dl class="simple">
<dt>[1] Koller &amp; Friedman, Probabilistic Graphical Models - Principles and Techniques, 2009,</dt><dd><p>Section 18.3.4–18.3.6 (esp. page 802).</p>
</dd>
<dt>[2] AM Carvalho, Scoring functions for learning Bayesian networks,</dt><dd><p><a class="reference external" href="http://www.lx.it.pt/~asmc/pub/talks/09-TA/ta_pres.pdf">http://www.lx.it.pt/~asmc/pub/talks/09-TA/ta_pres.pdf</a></p>
</dd>
</dl>
<dl class="py method">
<dt class="sig sig-object py" id="pgmpy.estimators.AIC.local_score">
<span class="sig-name descname"><span class="pre">local_score</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">variable</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">parents</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/pgmpy/estimators/StructureScore.html#AIC.local_score"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#pgmpy.estimators.AIC.local_score" title="Link to this definition">¶</a></dt>
<dd><p>Computes the local AIC score for a variable and its parent variables.</p>
<p>This method quantifies the fit of a variable to its parent set in the network,
balancing log-likelihood with a complexity penalty to avoid overfitting.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters<span class="colon">:</span></dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>variable</strong> (<em>str</em>) – The name of the variable (node) for which the local score is to be computed.</p></li>
<li><p><strong>parents</strong> (<em>list</em><em> of </em><em>str</em>) – List of variable names considered as parents of <cite>variable</cite>.</p></li>
</ul>
</dd>
<dt class="field-even">Returns<span class="colon">:</span></dt>
<dd class="field-even"><p><strong>score</strong> – The local AIC score for the specified variable and parent configuration.</p>
</dd>
<dt class="field-odd">Return type<span class="colon">:</span></dt>
<dd class="field-odd"><p>float</p>
</dd>
</dl>
<p class="rubric">Examples</p>
<div class="doctest highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">variable</span> <span class="o">=</span> <span class="s2">&quot;B&quot;</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">parents</span> <span class="o">=</span> <span class="p">[</span><span class="s2">&quot;A&quot;</span><span class="p">]</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">score</span> <span class="o">=</span> <span class="n">aic_score</span><span class="o">.</span><span class="n">local_score</span><span class="p">(</span><span class="n">variable</span><span class="p">,</span> <span class="n">parents</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="nb">print</span><span class="p">(</span><span class="n">score</span><span class="p">)</span>
<span class="go">-17.032</span>
</pre></div>
</div>
<dl class="field-list simple">
<dt class="field-odd">Raises<span class="colon">:</span></dt>
<dd class="field-odd"><p><strong>ValueError</strong> – If <cite>variable</cite> or any parent is not present in <cite>state_names</cite> or data, or if
    the data contains unsupported types (e.g., continuous values).</p>
</dd>
</dl>
</dd></dl>

</dd></dl>

</section>
<section id="k2-score">
<h2>K2 Score<a class="headerlink" href="#k2-score" title="Link to this heading">¶</a></h2>
<dl class="py class">
<dt class="sig sig-object py" id="pgmpy.estimators.K2">
<em class="property"><span class="k"><span class="pre">class</span></span><span class="w"> </span></em><span class="sig-prename descclassname"><span class="pre">pgmpy.estimators.</span></span><span class="sig-name descname"><span class="pre">K2</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">data</span></span></em>, <em class="sig-param"><span class="o"><span class="pre">**</span></span><span class="n"><span class="pre">kwargs</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/pgmpy/estimators/StructureScore.html#K2"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#pgmpy.estimators.K2" title="Link to this definition">¶</a></dt>
<dd><p>K2 structure score for discrete Bayesian networks using Dirichlet priors.</p>
<p>The K2 score is commonly used to evaluate the fit of a Bayesian network structure
on fully discrete data, assuming all Dirichlet hyperparameters (pseudo-counts) are set to 1.
This metric is suitable for structure learning when variables are categorical and no
prior preference for particular parameterizations is assumed.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters<span class="colon">:</span></dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>data</strong> (<em>pandas.DataFrame</em>) – DataFrame where each column represents a discrete variable. Missing values
should be set to <cite>numpy.nan</cite>. (Note: pandas will convert columns with <cite>numpy.nan</cite> to dtype float.)</p></li>
<li><p><strong>state_names</strong> (<em>dict</em><em>, </em><em>optional</em>) – Dictionary mapping each variable to its discrete states. If not specified, the unique
values observed in the data are used as possible states.</p></li>
</ul>
</dd>
</dl>
<p class="rubric">Examples</p>
<div class="doctest highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="kn">import</span> <span class="nn">pandas</span> <span class="k">as</span> <span class="nn">pd</span>
<span class="gp">&gt;&gt;&gt; </span><span class="kn">import</span> <span class="nn">numpy</span> <span class="k">as</span> <span class="nn">np</span>
<span class="gp">&gt;&gt;&gt; </span><span class="kn">from</span> <span class="nn">pgmpy.models</span> <span class="kn">import</span> <span class="n">DiscreteBayesianNetwork</span>
<span class="gp">&gt;&gt;&gt; </span><span class="kn">from</span> <span class="nn">pgmpy.estimators</span> <span class="kn">import</span> <span class="n">K2</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">data</span> <span class="o">=</span> <span class="n">pd</span><span class="o">.</span><span class="n">DataFrame</span><span class="p">({</span><span class="s2">&quot;A&quot;</span><span class="p">:</span> <span class="p">[</span><span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">0</span><span class="p">],</span> <span class="s2">&quot;B&quot;</span><span class="p">:</span> <span class="p">[</span><span class="mi">1</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">0</span><span class="p">],</span> <span class="s2">&quot;C&quot;</span><span class="p">:</span> <span class="p">[</span><span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">0</span><span class="p">]})</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">model</span> <span class="o">=</span> <span class="n">DiscreteBayesianNetwork</span><span class="p">([(</span><span class="s2">&quot;A&quot;</span><span class="p">,</span> <span class="s2">&quot;B&quot;</span><span class="p">),</span> <span class="p">(</span><span class="s2">&quot;A&quot;</span><span class="p">,</span> <span class="s2">&quot;C&quot;</span><span class="p">)])</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">k2_score</span> <span class="o">=</span> <span class="n">K2</span><span class="p">(</span><span class="n">data</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="nb">print</span><span class="p">(</span><span class="n">k2_score</span><span class="o">.</span><span class="n">score</span><span class="p">(</span><span class="n">model</span><span class="p">))</span>
<span class="go">-356.1785</span>
</pre></div>
</div>
<dl class="field-list simple">
<dt class="field-odd">Raises<span class="colon">:</span></dt>
<dd class="field-odd"><p><strong>ValueError</strong> – If the data contains continuous variables, or if the model variables are not present in the data.</p>
</dd>
</dl>
<p class="rubric">References</p>
<dl class="simple">
<dt>[1] Koller &amp; Friedman, Probabilistic Graphical Models - Principles and Techniques, 2009,</dt><dd><p>Section 18.3.4–18.3.6 (esp. page 806).</p>
</dd>
<dt>[2] AM Carvalho, Scoring functions for learning Bayesian networks,</dt><dd><p><a class="reference external" href="http://www.lx.it.pt/~asmc/pub/talks/09-TA/ta_pres.pdf">http://www.lx.it.pt/~asmc/pub/talks/09-TA/ta_pres.pdf</a></p>
</dd>
</dl>
<dl class="py method">
<dt class="sig sig-object py" id="pgmpy.estimators.K2.local_score">
<span class="sig-name descname"><span class="pre">local_score</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">variable</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">parents</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/pgmpy/estimators/StructureScore.html#K2.local_score"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#pgmpy.estimators.K2.local_score" title="Link to this definition">¶</a></dt>
<dd><p>Computes the local K2 score for a discrete variable and its parent variables.</p>
<p>The K2 local score measures how well the conditional probability distribution
of <cite>variable</cite> given its parents fits the observed data, assuming uniform Dirichlet
priors (all hyperparameters set to 1). The calculation is based on marginal and
conditional counts, and is suitable for fully discrete Bayesian networks.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters<span class="colon">:</span></dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>variable</strong> (<em>str</em>) – The name of the target variable (child node).</p></li>
<li><p><strong>parents</strong> (<em>list</em><em> of </em><em>str</em>) – List of parent variable names (categorical/discrete).</p></li>
</ul>
</dd>
<dt class="field-even">Returns<span class="colon">:</span></dt>
<dd class="field-even"><p><strong>score</strong> – The local K2 score for the specified variable and parent configuration.</p>
</dd>
<dt class="field-odd">Return type<span class="colon">:</span></dt>
<dd class="field-odd"><p>float</p>
</dd>
</dl>
<p class="rubric">Examples</p>
<div class="doctest highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">variable</span> <span class="o">=</span> <span class="s2">&quot;B&quot;</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">parents</span> <span class="o">=</span> <span class="p">[</span><span class="s2">&quot;A&quot;</span><span class="p">]</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">s</span> <span class="o">=</span> <span class="n">k2_score</span><span class="o">.</span><span class="n">local_score</span><span class="p">(</span><span class="n">variable</span><span class="p">,</span> <span class="n">parents</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="nb">print</span><span class="p">(</span><span class="n">s</span><span class="p">)</span>
<span class="go">-42.18</span>
</pre></div>
</div>
<dl class="field-list simple">
<dt class="field-odd">Raises<span class="colon">:</span></dt>
<dd class="field-odd"><p><strong>ValueError</strong> – If <cite>variable</cite> or any parent is not present in <cite>state_names</cite> or data, or if the data
    is not fully discrete.</p>
</dd>
</dl>
<p class="rubric">References</p>
<dl class="simple">
<dt>[1] Koller &amp; Friedman, Probabilistic Graphical Models - Principles and Techniques, 2009,</dt><dd><p>Section 18.3.4–18.3.6 (esp. page 806).</p>
</dd>
</dl>
</dd></dl>

</dd></dl>

</section>
<section id="bds-score">
<h2>BDs Score<a class="headerlink" href="#bds-score" title="Link to this heading">¶</a></h2>
<dl class="py class">
<dt class="sig sig-object py" id="pgmpy.estimators.BDs">
<em class="property"><span class="k"><span class="pre">class</span></span><span class="w"> </span></em><span class="sig-prename descclassname"><span class="pre">pgmpy.estimators.</span></span><span class="sig-name descname"><span class="pre">BDs</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">data</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">equivalent_sample_size</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">10</span></span></em>, <em class="sig-param"><span class="o"><span class="pre">**</span></span><span class="n"><span class="pre">kwargs</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/pgmpy/estimators/StructureScore.html#BDs"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#pgmpy.estimators.BDs" title="Link to this definition">¶</a></dt>
<dd><p>BDs (Bayesian Dirichlet sparse) structure score for discrete Bayesian networks.</p>
<p>The BDs score is a variant of the BDeu score that sets Dirichlet hyperparameters
(pseudo-counts) proportional to the number of observed parent configurations,
leading to improved scoring in sparse or partially observed data scenarios.</p>
<p>Use this score when you expect many possible parent configurations in your data
to be unobserved (common in sparse or high-dimensional discrete datasets).</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters<span class="colon">:</span></dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>data</strong> (<em>pandas.DataFrame</em>) – DataFrame where each column represents a discrete variable.
Missing values should be set as <cite>numpy.nan</cite>.
Note: pandas converts such columns to dtype float.</p></li>
<li><p><strong>equivalent_sample_size</strong> (<em>int</em><em>, </em><em>optional</em><em> (</em><em>default: 10</em><em>)</em>) – The equivalent (imaginary) sample size for the Dirichlet hyperparameters.
The score is sensitive to this value; try different values if needed.</p></li>
<li><p><strong>state_names</strong> (<em>dict</em><em>, </em><em>optional</em>) – Dictionary mapping variable names to their discrete states.
If not specified, unique values observed in the data are used as possible states.</p></li>
</ul>
</dd>
</dl>
<p class="rubric">Examples</p>
<div class="doctest highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="kn">import</span> <span class="nn">pandas</span> <span class="k">as</span> <span class="nn">pd</span>
<span class="gp">&gt;&gt;&gt; </span><span class="kn">import</span> <span class="nn">numpy</span> <span class="k">as</span> <span class="nn">np</span>
<span class="gp">&gt;&gt;&gt; </span><span class="kn">from</span> <span class="nn">pgmpy.models</span> <span class="kn">import</span> <span class="n">DiscreteBayesianNetwork</span>
<span class="gp">&gt;&gt;&gt; </span><span class="kn">from</span> <span class="nn">pgmpy.estimators</span> <span class="kn">import</span> <span class="n">BDs</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">data</span> <span class="o">=</span> <span class="n">pd</span><span class="o">.</span><span class="n">DataFrame</span><span class="p">({</span><span class="s2">&quot;A&quot;</span><span class="p">:</span> <span class="p">[</span><span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">0</span><span class="p">],</span> <span class="s2">&quot;B&quot;</span><span class="p">:</span> <span class="p">[</span><span class="mi">1</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">0</span><span class="p">],</span> <span class="s2">&quot;C&quot;</span><span class="p">:</span> <span class="p">[</span><span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">0</span><span class="p">]})</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">model</span> <span class="o">=</span> <span class="n">DiscreteBayesianNetwork</span><span class="p">([(</span><span class="s2">&quot;A&quot;</span><span class="p">,</span> <span class="s2">&quot;B&quot;</span><span class="p">),</span> <span class="p">(</span><span class="s2">&quot;A&quot;</span><span class="p">,</span> <span class="s2">&quot;C&quot;</span><span class="p">)])</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">bds_score</span> <span class="o">=</span> <span class="n">BDs</span><span class="p">(</span><span class="n">data</span><span class="p">,</span> <span class="n">equivalent_sample_size</span><span class="o">=</span><span class="mi">5</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="nb">print</span><span class="p">(</span><span class="n">bds_score</span><span class="o">.</span><span class="n">score</span><span class="p">(</span><span class="n">model</span><span class="p">))</span>
<span class="go">-210.314</span>
</pre></div>
</div>
<dl class="field-list simple">
<dt class="field-odd">Raises<span class="colon">:</span></dt>
<dd class="field-odd"><p><strong>ValueError</strong> – If the data contains continuous variables, or if the model variables are not present in the data.</p>
</dd>
</dl>
<p class="rubric">References</p>
<dl class="simple">
<dt>[1] Scutari, Marco. An Empirical-Bayes Score for Discrete Bayesian Networks.</dt><dd><p>Journal of Machine Learning Research, 2016, pp. 438–48</p>
</dd>
</dl>
<dl class="py method">
<dt class="sig sig-object py" id="pgmpy.estimators.BDs.local_score">
<span class="sig-name descname"><span class="pre">local_score</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">variable</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">parents</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/pgmpy/estimators/StructureScore.html#BDs.local_score"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#pgmpy.estimators.BDs.local_score" title="Link to this definition">¶</a></dt>
<dd><p>Computes the local BDs score for a variable and its parent variables.</p>
<p>The BDs local score quantifies how well the given variable is explained by its
specified parent set, using a Bayesian Dirichlet sparse prior. The hyperparameters
are adjusted based on the number of observed parent configurations, making the score
more robust in sparse data scenarios.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters<span class="colon">:</span></dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>variable</strong> (<em>str</em>) – The name of the variable (node) for which the local score is to be computed.</p></li>
<li><p><strong>parents</strong> (<em>list</em><em> of </em><em>str</em>) – List of variable names considered as parents of <cite>variable</cite>.</p></li>
</ul>
</dd>
<dt class="field-even">Returns<span class="colon">:</span></dt>
<dd class="field-even"><p><strong>score</strong> – The local BDs score for the specified variable and parent configuration.</p>
</dd>
<dt class="field-odd">Return type<span class="colon">:</span></dt>
<dd class="field-odd"><p>float</p>
</dd>
</dl>
<p class="rubric">Examples</p>
<div class="doctest highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">variable</span> <span class="o">=</span> <span class="s2">&quot;B&quot;</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">parents</span> <span class="o">=</span> <span class="p">[</span><span class="s2">&quot;A&quot;</span><span class="p">]</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">score</span> <span class="o">=</span> <span class="n">bds_score</span><span class="o">.</span><span class="n">local_score</span><span class="p">(</span><span class="n">variable</span><span class="p">,</span> <span class="n">parents</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="nb">print</span><span class="p">(</span><span class="n">score</span><span class="p">)</span>
<span class="go">-38.215</span>
</pre></div>
</div>
<dl class="field-list simple">
<dt class="field-odd">Raises<span class="colon">:</span></dt>
<dd class="field-odd"><p><strong>ValueError</strong> – If <cite>variable</cite> or any parent is not present in <cite>state_names</cite> or data, or if
    the data contains unsupported types (e.g., continuous values).</p>
</dd>
</dl>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="pgmpy.estimators.BDs.structure_prior">
<span class="sig-name descname"><span class="pre">structure_prior</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">model</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/pgmpy/estimators/StructureScore.html#BDs.structure_prior"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#pgmpy.estimators.BDs.structure_prior" title="Link to this definition">¶</a></dt>
<dd><p>Computes the marginal uniform prior for a Bayesian network structure.</p>
<p>This method assigns a marginal uniform prior to the graph structure, where
the probability of an arc (edge) between any two nodes (in either direction) is 1/4,
and the probability of no arc between any two nodes is 1/2. The returned value
is the log prior probability for the given model structure.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters<span class="colon">:</span></dt>
<dd class="field-odd"><p><strong>model</strong> (<a class="reference internal" href="../models/bayesiannetwork.html#pgmpy.models.DiscreteBayesianNetwork.DiscreteBayesianNetwork" title="pgmpy.models.DiscreteBayesianNetwork.DiscreteBayesianNetwork"><em>DiscreteBayesianNetwork</em></a>) – The Bayesian network model for which to compute the structure prior.</p>
</dd>
<dt class="field-even">Returns<span class="colon">:</span></dt>
<dd class="field-even"><p><strong>score</strong> – The log prior probability of the given network structure under the marginal uniform prior.</p>
</dd>
<dt class="field-odd">Return type<span class="colon">:</span></dt>
<dd class="field-odd"><p>float</p>
</dd>
</dl>
<p class="rubric">Examples</p>
<div class="doctest highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="kn">from</span> <span class="nn">pgmpy.models</span> <span class="kn">import</span> <span class="n">DiscreteBayesianNetwork</span>
<span class="gp">&gt;&gt;&gt; </span><span class="kn">from</span> <span class="nn">pgmpy.estimators</span> <span class="kn">import</span> <span class="n">BDs</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">model</span> <span class="o">=</span> <span class="n">DiscreteBayesianNetwork</span><span class="p">([(</span><span class="s2">&quot;A&quot;</span><span class="p">,</span> <span class="s2">&quot;B&quot;</span><span class="p">),</span> <span class="p">(</span><span class="s2">&quot;C&quot;</span><span class="p">,</span> <span class="s2">&quot;D&quot;</span><span class="p">)])</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">score</span> <span class="o">=</span> <span class="n">BDs</span><span class="p">(</span><span class="n">data</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">prior</span> <span class="o">=</span> <span class="n">score</span><span class="o">.</span><span class="n">structure_prior</span><span class="p">(</span><span class="n">model</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="nb">print</span><span class="p">(</span><span class="n">prior</span><span class="p">)</span>
<span class="go">-4.1588830833596715</span>
</pre></div>
</div>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="pgmpy.estimators.BDs.structure_prior_ratio">
<span class="sig-name descname"><span class="pre">structure_prior_ratio</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">operation</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/pgmpy/estimators/StructureScore.html#BDs.structure_prior_ratio"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#pgmpy.estimators.BDs.structure_prior_ratio" title="Link to this definition">¶</a></dt>
<dd><p>Computes the log ratio of prior probabilities for a proposed change to the DAG structure.</p>
<p>This method implements the marginal uniform prior for the graph structure, where the
log prior probability ratio is -log(2) for adding an edge, log(2) for removing an edge,
and 0 otherwise.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters<span class="colon">:</span></dt>
<dd class="field-odd"><p><strong>operation</strong> (<em>str</em>) – The proposed operation on the Directed Acyclic Graph (DAG).
Use “+” for adding an edge, “-” for removing an edge, or other values for no change.</p>
</dd>
<dt class="field-even">Returns<span class="colon">:</span></dt>
<dd class="field-even"><p><strong>prior_ratio</strong> – The log ratio of the prior probabilities for the proposed operation.</p>
</dd>
<dt class="field-odd">Return type<span class="colon">:</span></dt>
<dd class="field-odd"><p>float</p>
</dd>
</dl>
<p class="rubric">Examples</p>
<div class="doctest highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="kn">from</span> <span class="nn">pgmpy.estimators</span> <span class="kn">import</span> <span class="n">BDs</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">score</span> <span class="o">=</span> <span class="n">BDs</span><span class="p">(</span><span class="n">data</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">score</span><span class="o">.</span><span class="n">structure_prior_ratio</span><span class="p">(</span><span class="s2">&quot;+&quot;</span><span class="p">)</span>
<span class="go">-0.6931471805599453</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">score</span><span class="o">.</span><span class="n">structure_prior_ratio</span><span class="p">(</span><span class="s2">&quot;-&quot;</span><span class="p">)</span>
<span class="go">0.6931471805599453</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">score</span><span class="o">.</span><span class="n">structure_prior_ratio</span><span class="p">(</span><span class="s2">&quot;noop&quot;</span><span class="p">)</span>
<span class="go">0</span>
</pre></div>
</div>
</dd></dl>

</dd></dl>

</section>
<section id="gaussian-log-likelihood-score">
<h2>Gaussian Log-Likelihood Score<a class="headerlink" href="#gaussian-log-likelihood-score" title="Link to this heading">¶</a></h2>
<dl class="py class">
<dt class="sig sig-object py" id="pgmpy.estimators.LogLikelihoodGauss">
<em class="property"><span class="k"><span class="pre">class</span></span><span class="w"> </span></em><span class="sig-prename descclassname"><span class="pre">pgmpy.estimators.</span></span><span class="sig-name descname"><span class="pre">LogLikelihoodGauss</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">data</span></span></em>, <em class="sig-param"><span class="o"><span class="pre">**</span></span><span class="n"><span class="pre">kwargs</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/pgmpy/estimators/StructureScore.html#LogLikelihoodGauss"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#pgmpy.estimators.LogLikelihoodGauss" title="Link to this definition">¶</a></dt>
<dd><p>Log-likelihood structure score for Gaussian Bayesian networks.</p>
<p>This score evaluates the fit of a continuous (Gaussian) Bayesian network structure
by computing the (unpenalized) log-likelihood of the observed data given the model,
using generalized linear modeling. It is suitable for networks with continuous variables.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters<span class="colon">:</span></dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>data</strong> (<em>pandas.DataFrame</em>) – DataFrame where each column represents a continuous variable.</p></li>
<li><p><strong>state_names</strong> (<em>dict</em><em>, </em><em>optional</em>) – Dictionary mapping variable names to possible states. Not typically used for Gaussian networks.</p></li>
</ul>
</dd>
</dl>
<p class="rubric">Examples</p>
<div class="doctest highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="kn">import</span> <span class="nn">pandas</span> <span class="k">as</span> <span class="nn">pd</span>
<span class="gp">&gt;&gt;&gt; </span><span class="kn">import</span> <span class="nn">numpy</span> <span class="k">as</span> <span class="nn">np</span>
<span class="gp">&gt;&gt;&gt; </span><span class="kn">from</span> <span class="nn">pgmpy.estimators</span> <span class="kn">import</span> <span class="n">LogLikelihoodGauss</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">data</span> <span class="o">=</span> <span class="n">pd</span><span class="o">.</span><span class="n">DataFrame</span><span class="p">(</span>
<span class="gp">... </span>    <span class="p">{</span>
<span class="gp">... </span>        <span class="s2">&quot;A&quot;</span><span class="p">:</span> <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="mi">100</span><span class="p">),</span>
<span class="gp">... </span>        <span class="s2">&quot;B&quot;</span><span class="p">:</span> <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="mi">100</span><span class="p">),</span>
<span class="gp">... </span>        <span class="s2">&quot;C&quot;</span><span class="p">:</span> <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="mi">100</span><span class="p">),</span>
<span class="gp">... </span>    <span class="p">}</span>
<span class="gp">... </span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">score</span> <span class="o">=</span> <span class="n">LogLikelihoodGauss</span><span class="p">(</span><span class="n">data</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">ll</span> <span class="o">=</span> <span class="n">score</span><span class="o">.</span><span class="n">local_score</span><span class="p">(</span><span class="s2">&quot;B&quot;</span><span class="p">,</span> <span class="p">[</span><span class="s2">&quot;A&quot;</span><span class="p">,</span> <span class="s2">&quot;C&quot;</span><span class="p">])</span>
<span class="gp">&gt;&gt;&gt; </span><span class="nb">print</span><span class="p">(</span><span class="n">ll</span><span class="p">)</span>
<span class="go">-142.125</span>
</pre></div>
</div>
<dl class="field-list simple">
<dt class="field-odd">Raises<span class="colon">:</span></dt>
<dd class="field-odd"><p><strong>ValueError</strong> – If the data contains discrete or non-numeric variables.</p>
</dd>
</dl>
<dl class="py method">
<dt class="sig sig-object py" id="pgmpy.estimators.LogLikelihoodGauss.local_score">
<span class="sig-name descname"><span class="pre">local_score</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">variable</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">parents</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/pgmpy/estimators/StructureScore.html#LogLikelihoodGauss.local_score"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#pgmpy.estimators.LogLikelihoodGauss.local_score" title="Link to this definition">¶</a></dt>
<dd><p>Computes the log-likelihood score for a variable given its parent variables.</p>
<p>Fits a generalized linear model (GLM) for the variable as a function of its parents,
and returns the resulting log-likelihood as the structure score.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters<span class="colon">:</span></dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>variable</strong> (<em>str</em>) – The name of the variable (node) for which the local score is to be computed.</p></li>
<li><p><strong>parents</strong> (<em>list</em><em> of </em><em>str</em>) – List of variable names considered as parents of <cite>variable</cite>.</p></li>
</ul>
</dd>
<dt class="field-even">Returns<span class="colon">:</span></dt>
<dd class="field-even"><p><strong>score</strong> – The log-likelihood score for the specified variable and parent configuration.</p>
</dd>
<dt class="field-odd">Return type<span class="colon">:</span></dt>
<dd class="field-odd"><p>float</p>
</dd>
</dl>
<p class="rubric">Examples</p>
<div class="doctest highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">ll</span> <span class="o">=</span> <span class="n">score</span><span class="o">.</span><span class="n">local_score</span><span class="p">(</span><span class="s2">&quot;B&quot;</span><span class="p">,</span> <span class="p">[</span><span class="s2">&quot;A&quot;</span><span class="p">,</span> <span class="s2">&quot;C&quot;</span><span class="p">])</span>
<span class="gp">&gt;&gt;&gt; </span><span class="nb">print</span><span class="p">(</span><span class="n">ll</span><span class="p">)</span>
<span class="go">-142.125</span>
</pre></div>
</div>
<dl class="field-list simple">
<dt class="field-odd">Raises<span class="colon">:</span></dt>
<dd class="field-odd"><p><strong>ValueError</strong> – If the GLM cannot be fitted due to non-numeric data or missing columns.</p>
</dd>
</dl>
</dd></dl>

</dd></dl>

</section>
<section id="gaussian-bic-score">
<h2>Gaussian BIC Score<a class="headerlink" href="#gaussian-bic-score" title="Link to this heading">¶</a></h2>
<dl class="py class">
<dt class="sig sig-object py" id="pgmpy.estimators.BICGauss">
<em class="property"><span class="k"><span class="pre">class</span></span><span class="w"> </span></em><span class="sig-prename descclassname"><span class="pre">pgmpy.estimators.</span></span><span class="sig-name descname"><span class="pre">BICGauss</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">data</span></span></em>, <em class="sig-param"><span class="o"><span class="pre">**</span></span><span class="n"><span class="pre">kwargs</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/pgmpy/estimators/StructureScore.html#BICGauss"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#pgmpy.estimators.BICGauss" title="Link to this definition">¶</a></dt>
<dd><p>BIC (Bayesian Information Criterion) structure score for Gaussian Bayesian networks.</p>
<p>The BICGauss score evaluates continuous Bayesian network structures by penalizing
the log-likelihood with a term proportional to the number of model parameters,
discouraging overfitting. This is the Gaussian version of the BIC/MDL score,
suitable for networks where all variables are continuous.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters<span class="colon">:</span></dt>
<dd class="field-odd"><p><strong>data</strong> (<em>pandas.DataFrame</em>) – DataFrame where each column represents a continuous variable.</p>
</dd>
</dl>
<p class="rubric">Examples</p>
<div class="doctest highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="kn">import</span> <span class="nn">pandas</span> <span class="k">as</span> <span class="nn">pd</span>
<span class="gp">&gt;&gt;&gt; </span><span class="kn">import</span> <span class="nn">numpy</span> <span class="k">as</span> <span class="nn">np</span>
<span class="gp">&gt;&gt;&gt; </span><span class="kn">from</span> <span class="nn">pgmpy.estimators</span> <span class="kn">import</span> <span class="n">BICGauss</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">data</span> <span class="o">=</span> <span class="n">pd</span><span class="o">.</span><span class="n">DataFrame</span><span class="p">(</span>
<span class="gp">... </span>    <span class="p">{</span>
<span class="gp">... </span>        <span class="s2">&quot;A&quot;</span><span class="p">:</span> <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="mi">100</span><span class="p">),</span>
<span class="gp">... </span>        <span class="s2">&quot;B&quot;</span><span class="p">:</span> <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="mi">100</span><span class="p">),</span>
<span class="gp">... </span>        <span class="s2">&quot;C&quot;</span><span class="p">:</span> <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="mi">100</span><span class="p">),</span>
<span class="gp">... </span>    <span class="p">}</span>
<span class="gp">... </span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">score</span> <span class="o">=</span> <span class="n">BICGauss</span><span class="p">(</span><span class="n">data</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">s</span> <span class="o">=</span> <span class="n">score</span><span class="o">.</span><span class="n">local_score</span><span class="p">(</span><span class="s2">&quot;B&quot;</span><span class="p">,</span> <span class="p">[</span><span class="s2">&quot;A&quot;</span><span class="p">,</span> <span class="s2">&quot;C&quot;</span><span class="p">])</span>
<span class="gp">&gt;&gt;&gt; </span><span class="nb">print</span><span class="p">(</span><span class="n">s</span><span class="p">)</span>
<span class="go">-111.42</span>
</pre></div>
</div>
<dl class="field-list simple">
<dt class="field-odd">Raises<span class="colon">:</span></dt>
<dd class="field-odd"><p><strong>ValueError</strong> – If the GLM cannot be fitted due to missing or non-numeric data.</p>
</dd>
</dl>
<dl class="py method">
<dt class="sig sig-object py" id="pgmpy.estimators.BICGauss.local_score">
<span class="sig-name descname"><span class="pre">local_score</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">variable</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">parents</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/pgmpy/estimators/StructureScore.html#BICGauss.local_score"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#pgmpy.estimators.BICGauss.local_score" title="Link to this definition">¶</a></dt>
<dd><p>Computes the local BIC/MDL score for a variable and its parent variables
in a Gaussian Bayesian network.</p>
<p>The score is the log-likelihood minus a penalty term that increases
with the number of model parameters and sample size.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters<span class="colon">:</span></dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>variable</strong> (<em>str</em>) – The name of the variable (node) for which the local score is to be computed.</p></li>
<li><p><strong>parents</strong> (<em>list</em><em> of </em><em>str</em>) – List of variable names considered as parents of <cite>variable</cite>.</p></li>
</ul>
</dd>
<dt class="field-even">Returns<span class="colon">:</span></dt>
<dd class="field-even"><p><strong>score</strong> – The local BICGauss score for the specified variable and parent configuration.</p>
</dd>
<dt class="field-odd">Return type<span class="colon">:</span></dt>
<dd class="field-odd"><p>float</p>
</dd>
</dl>
<p class="rubric">Examples</p>
<div class="doctest highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">s</span> <span class="o">=</span> <span class="n">score</span><span class="o">.</span><span class="n">local_score</span><span class="p">(</span><span class="s2">&quot;B&quot;</span><span class="p">,</span> <span class="p">[</span><span class="s2">&quot;A&quot;</span><span class="p">,</span> <span class="s2">&quot;C&quot;</span><span class="p">])</span>
<span class="gp">&gt;&gt;&gt; </span><span class="nb">print</span><span class="p">(</span><span class="n">s</span><span class="p">)</span>
<span class="go">-111.42</span>
</pre></div>
</div>
<dl class="field-list simple">
<dt class="field-odd">Raises<span class="colon">:</span></dt>
<dd class="field-odd"><p><strong>ValueError</strong> – If the GLM cannot be fitted due to missing or non-numeric data.</p>
</dd>
</dl>
</dd></dl>

</dd></dl>

</section>
<section id="gaussian-aic-score">
<h2>Gaussian AIC Score<a class="headerlink" href="#gaussian-aic-score" title="Link to this heading">¶</a></h2>
<dl class="py class">
<dt class="sig sig-object py" id="pgmpy.estimators.AICGauss">
<em class="property"><span class="k"><span class="pre">class</span></span><span class="w"> </span></em><span class="sig-prename descclassname"><span class="pre">pgmpy.estimators.</span></span><span class="sig-name descname"><span class="pre">AICGauss</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">data</span></span></em>, <em class="sig-param"><span class="o"><span class="pre">**</span></span><span class="n"><span class="pre">kwargs</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/pgmpy/estimators/StructureScore.html#AICGauss"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#pgmpy.estimators.AICGauss" title="Link to this definition">¶</a></dt>
<dd><p>AIC (Akaike Information Criterion) structure score for Gaussian Bayesian networks.</p>
<p>The AICGauss score evaluates continuous Bayesian network structures by penalizing
the log-likelihood with a term proportional to the number of model parameters.
The penalty is less severe than BIC and does not depend on sample size, making AIC
preferable for model selection with smaller datasets.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters<span class="colon">:</span></dt>
<dd class="field-odd"><p><strong>data</strong> (<em>pandas.DataFrame</em>) – DataFrame where each column represents a continuous variable.</p>
</dd>
</dl>
<p class="rubric">Examples</p>
<div class="doctest highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="kn">import</span> <span class="nn">pandas</span> <span class="k">as</span> <span class="nn">pd</span>
<span class="gp">&gt;&gt;&gt; </span><span class="kn">import</span> <span class="nn">numpy</span> <span class="k">as</span> <span class="nn">np</span>
<span class="gp">&gt;&gt;&gt; </span><span class="kn">from</span> <span class="nn">pgmpy.estimators</span> <span class="kn">import</span> <span class="n">AICGauss</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">data</span> <span class="o">=</span> <span class="n">pd</span><span class="o">.</span><span class="n">DataFrame</span><span class="p">(</span>
<span class="gp">... </span>    <span class="p">{</span>
<span class="gp">... </span>        <span class="s2">&quot;A&quot;</span><span class="p">:</span> <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="mi">100</span><span class="p">),</span>
<span class="gp">... </span>        <span class="s2">&quot;B&quot;</span><span class="p">:</span> <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="mi">100</span><span class="p">),</span>
<span class="gp">... </span>        <span class="s2">&quot;C&quot;</span><span class="p">:</span> <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="mi">100</span><span class="p">),</span>
<span class="gp">... </span>    <span class="p">}</span>
<span class="gp">... </span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">score</span> <span class="o">=</span> <span class="n">AICGauss</span><span class="p">(</span><span class="n">data</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">s</span> <span class="o">=</span> <span class="n">score</span><span class="o">.</span><span class="n">local_score</span><span class="p">(</span><span class="s2">&quot;B&quot;</span><span class="p">,</span> <span class="p">[</span><span class="s2">&quot;A&quot;</span><span class="p">,</span> <span class="s2">&quot;C&quot;</span><span class="p">])</span>
<span class="gp">&gt;&gt;&gt; </span><span class="nb">print</span><span class="p">(</span><span class="n">s</span><span class="p">)</span>
<span class="go">-97.53</span>
</pre></div>
</div>
<dl class="field-list simple">
<dt class="field-odd">Raises<span class="colon">:</span></dt>
<dd class="field-odd"><p><strong>ValueError</strong> – If the GLM cannot be fitted due to missing or non-numeric data.</p>
</dd>
</dl>
<dl class="py method">
<dt class="sig sig-object py" id="pgmpy.estimators.AICGauss.local_score">
<span class="sig-name descname"><span class="pre">local_score</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">variable</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">parents</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/pgmpy/estimators/StructureScore.html#AICGauss.local_score"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#pgmpy.estimators.AICGauss.local_score" title="Link to this definition">¶</a></dt>
<dd><p>Computes the local AIC score for a variable and its parent variables
in a Gaussian Bayesian network.</p>
<p>The score is the log-likelihood minus a penalty term that increases with
the number of model parameters (but not sample size).</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters<span class="colon">:</span></dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>variable</strong> (<em>str</em>) – The name of the variable (node) for which the local score is to be computed.</p></li>
<li><p><strong>parents</strong> (<em>list</em><em> of </em><em>str</em>) – List of variable names considered as parents of <cite>variable</cite>.</p></li>
</ul>
</dd>
<dt class="field-even">Returns<span class="colon">:</span></dt>
<dd class="field-even"><p><strong>score</strong> – The local AICGauss score for the specified variable and parent configuration.</p>
</dd>
<dt class="field-odd">Return type<span class="colon">:</span></dt>
<dd class="field-odd"><p>float</p>
</dd>
</dl>
<p class="rubric">Examples</p>
<div class="doctest highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">s</span> <span class="o">=</span> <span class="n">score</span><span class="o">.</span><span class="n">local_score</span><span class="p">(</span><span class="s2">&quot;B&quot;</span><span class="p">,</span> <span class="p">[</span><span class="s2">&quot;A&quot;</span><span class="p">,</span> <span class="s2">&quot;C&quot;</span><span class="p">])</span>
<span class="gp">&gt;&gt;&gt; </span><span class="nb">print</span><span class="p">(</span><span class="n">s</span><span class="p">)</span>
<span class="go">-97.53</span>
</pre></div>
</div>
<dl class="field-list simple">
<dt class="field-odd">Raises<span class="colon">:</span></dt>
<dd class="field-odd"><p><strong>ValueError</strong> – If the GLM cannot be fitted due to missing or non-numeric data.</p>
</dd>
</dl>
</dd></dl>

</dd></dl>

</section>
<section id="conditional-gaussian-log-likelihood-score">
<h2>Conditional Gaussian Log-Likelihood Score<a class="headerlink" href="#conditional-gaussian-log-likelihood-score" title="Link to this heading">¶</a></h2>
<dl class="py class">
<dt class="sig sig-object py" id="pgmpy.estimators.LogLikelihoodCondGauss">
<em class="property"><span class="k"><span class="pre">class</span></span><span class="w"> </span></em><span class="sig-prename descclassname"><span class="pre">pgmpy.estimators.</span></span><span class="sig-name descname"><span class="pre">LogLikelihoodCondGauss</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">data</span></span></em>, <em class="sig-param"><span class="o"><span class="pre">**</span></span><span class="n"><span class="pre">kwargs</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/pgmpy/estimators/StructureScore.html#LogLikelihoodCondGauss"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#pgmpy.estimators.LogLikelihoodCondGauss" title="Link to this definition">¶</a></dt>
<dd><p>Log-likelihood score for Bayesian networks with mixed discrete and continuous variables.</p>
<p>This score is based on conditional Gaussian distributions and supports networks
with both discrete and continuous variables, using the methodology described in [1].
The local score computes the log-likelihood of the observed data given the
network structure, handling mixed parent sets as described in the reference.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters<span class="colon">:</span></dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>data</strong> (<em>pandas.DataFrame</em>) – DataFrame where columns can be discrete or continuous variables.
Variable types should be consistent with the structure.</p></li>
<li><p><strong>state_names</strong> (<em>dict</em><em>, </em><em>optional</em>) – Dictionary mapping discrete variable names to their possible states.</p></li>
</ul>
</dd>
</dl>
<p class="rubric">Examples</p>
<div class="doctest highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="kn">import</span> <span class="nn">pandas</span> <span class="k">as</span> <span class="nn">pd</span>
<span class="gp">&gt;&gt;&gt; </span><span class="kn">import</span> <span class="nn">numpy</span> <span class="k">as</span> <span class="nn">np</span>
<span class="gp">&gt;&gt;&gt; </span><span class="kn">from</span> <span class="nn">pgmpy.estimators</span> <span class="kn">import</span> <span class="n">LogLikelihoodCondGauss</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">data</span> <span class="o">=</span> <span class="n">pd</span><span class="o">.</span><span class="n">DataFrame</span><span class="p">(</span>
<span class="gp">... </span>    <span class="p">{</span>
<span class="gp">... </span>        <span class="s2">&quot;A&quot;</span><span class="p">:</span> <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="mi">100</span><span class="p">),</span>
<span class="gp">... </span>        <span class="s2">&quot;B&quot;</span><span class="p">:</span> <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">randint</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">100</span><span class="p">),</span>
<span class="gp">... </span>        <span class="s2">&quot;C&quot;</span><span class="p">:</span> <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="mi">100</span><span class="p">),</span>
<span class="gp">... </span>    <span class="p">}</span>
<span class="gp">... </span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">score</span> <span class="o">=</span> <span class="n">LogLikelihoodCondGauss</span><span class="p">(</span><span class="n">data</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">ll</span> <span class="o">=</span> <span class="n">score</span><span class="o">.</span><span class="n">local_score</span><span class="p">(</span><span class="s2">&quot;A&quot;</span><span class="p">,</span> <span class="p">[</span><span class="s2">&quot;B&quot;</span><span class="p">,</span> <span class="s2">&quot;C&quot;</span><span class="p">])</span>
<span class="gp">&gt;&gt;&gt; </span><span class="nb">print</span><span class="p">(</span><span class="n">ll</span><span class="p">)</span>
<span class="go">-98.452</span>
</pre></div>
</div>
<dl class="field-list simple">
<dt class="field-odd">Raises<span class="colon">:</span></dt>
<dd class="field-odd"><p><strong>ValueError</strong> – If the data or variable types are not suitable for conditional Gaussian modeling.</p>
</dd>
</dl>
<p class="rubric">References</p>
<dl class="simple">
<dt>[1] Andrews, B., Ramsey, J., &amp; Cooper, G. F. (2018). Scoring Bayesian</dt><dd><p>Networks of Mixed Variables. International journal of data science and
analytics, 6(1), 3–18. <a class="reference external" href="https://doi.org/10.1007/s41060-017-0085-7">https://doi.org/10.1007/s41060-017-0085-7</a></p>
</dd>
</dl>
<dl class="py method">
<dt class="sig sig-object py" id="pgmpy.estimators.LogLikelihoodCondGauss.local_score">
<span class="sig-name descname"><span class="pre">local_score</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">variable</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">parents</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/pgmpy/estimators/StructureScore.html#LogLikelihoodCondGauss.local_score"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#pgmpy.estimators.LogLikelihoodCondGauss.local_score" title="Link to this definition">¶</a></dt>
<dd><p>Computes the local log-likelihood score for a variable given its parent variables
in a mixed (discrete and continuous) Bayesian network.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters<span class="colon">:</span></dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>variable</strong> (<em>str</em>) – The name of the variable (node) for which the local score is to be computed.</p></li>
<li><p><strong>parents</strong> (<em>list</em><em> of </em><em>str</em>) – List of variable names considered as parents of <cite>variable</cite>.</p></li>
</ul>
</dd>
<dt class="field-even">Returns<span class="colon">:</span></dt>
<dd class="field-even"><p><strong>score</strong> – The local conditional Gaussian log-likelihood score for the specified variable and parent configuration.</p>
</dd>
<dt class="field-odd">Return type<span class="colon">:</span></dt>
<dd class="field-odd"><p>float</p>
</dd>
</dl>
<p class="rubric">Examples</p>
<div class="doctest highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">ll</span> <span class="o">=</span> <span class="n">score</span><span class="o">.</span><span class="n">local_score</span><span class="p">(</span><span class="s2">&quot;A&quot;</span><span class="p">,</span> <span class="p">[</span><span class="s2">&quot;B&quot;</span><span class="p">,</span> <span class="s2">&quot;C&quot;</span><span class="p">])</span>
<span class="gp">&gt;&gt;&gt; </span><span class="nb">print</span><span class="p">(</span><span class="n">ll</span><span class="p">)</span>
<span class="go">-98.452</span>
</pre></div>
</div>
<dl class="field-list simple">
<dt class="field-odd">Raises<span class="colon">:</span></dt>
<dd class="field-odd"><p><strong>ValueError</strong> – If the log-likelihood cannot be computed due to incompatible data or variable types.</p>
</dd>
</dl>
</dd></dl>

</dd></dl>

</section>
<section id="conditional-gaussian-bic-score">
<h2>Conditional Gaussian BIC Score<a class="headerlink" href="#conditional-gaussian-bic-score" title="Link to this heading">¶</a></h2>
<dl class="py class">
<dt class="sig sig-object py" id="pgmpy.estimators.BICCondGauss">
<em class="property"><span class="k"><span class="pre">class</span></span><span class="w"> </span></em><span class="sig-prename descclassname"><span class="pre">pgmpy.estimators.</span></span><span class="sig-name descname"><span class="pre">BICCondGauss</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">data</span></span></em>, <em class="sig-param"><span class="o"><span class="pre">**</span></span><span class="n"><span class="pre">kwargs</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/pgmpy/estimators/StructureScore.html#BICCondGauss"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#pgmpy.estimators.BICCondGauss" title="Link to this definition">¶</a></dt>
<dd><p>BIC (Bayesian Information Criterion) score for Bayesian networks with mixed (discrete and continuous) variables.</p>
<p>The BICCondGauss score evaluates network structures by penalizing the conditional log-likelihood
with a term proportional to the number of free parameters and the logarithm of sample size.
This approach generalizes the classic BIC to handle mixed discrete/continuous data as
described in [1].</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters<span class="colon">:</span></dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>data</strong> (<em>pandas.DataFrame</em>) – DataFrame where columns may be discrete or continuous variables.</p></li>
<li><p><strong>state_names</strong> (<em>dict</em><em>, </em><em>optional</em>) – Dictionary mapping discrete variable names to possible states.</p></li>
</ul>
</dd>
</dl>
<p class="rubric">Examples</p>
<div class="doctest highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="kn">import</span> <span class="nn">pandas</span> <span class="k">as</span> <span class="nn">pd</span>
<span class="gp">&gt;&gt;&gt; </span><span class="kn">import</span> <span class="nn">numpy</span> <span class="k">as</span> <span class="nn">np</span>
<span class="gp">&gt;&gt;&gt; </span><span class="kn">from</span> <span class="nn">pgmpy.estimators</span> <span class="kn">import</span> <span class="n">BICCondGauss</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">data</span> <span class="o">=</span> <span class="n">pd</span><span class="o">.</span><span class="n">DataFrame</span><span class="p">(</span>
<span class="gp">... </span>    <span class="p">{</span>
<span class="gp">... </span>        <span class="s2">&quot;A&quot;</span><span class="p">:</span> <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="mi">100</span><span class="p">),</span>
<span class="gp">... </span>        <span class="s2">&quot;B&quot;</span><span class="p">:</span> <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">randint</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">100</span><span class="p">),</span>
<span class="gp">... </span>        <span class="s2">&quot;C&quot;</span><span class="p">:</span> <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="mi">100</span><span class="p">),</span>
<span class="gp">... </span>    <span class="p">}</span>
<span class="gp">... </span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">score</span> <span class="o">=</span> <span class="n">BICCondGauss</span><span class="p">(</span><span class="n">data</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">s</span> <span class="o">=</span> <span class="n">score</span><span class="o">.</span><span class="n">local_score</span><span class="p">(</span><span class="s2">&quot;A&quot;</span><span class="p">,</span> <span class="p">[</span><span class="s2">&quot;B&quot;</span><span class="p">,</span> <span class="s2">&quot;C&quot;</span><span class="p">])</span>
<span class="gp">&gt;&gt;&gt; </span><span class="nb">print</span><span class="p">(</span><span class="n">s</span><span class="p">)</span>
<span class="go">-115.37</span>
</pre></div>
</div>
<dl class="field-list simple">
<dt class="field-odd">Raises<span class="colon">:</span></dt>
<dd class="field-odd"><p><strong>ValueError</strong> – If the log-likelihood or number of parameters cannot be computed for the provided variables.</p>
</dd>
</dl>
<p class="rubric">References</p>
<dl class="simple">
<dt>[1] Andrews, B., Ramsey, J., &amp; Cooper, G. F. (2018). Scoring Bayesian</dt><dd><p>Networks of Mixed Variables. International journal of data science and
analytics, 6(1), 3–18. <a class="reference external" href="https://doi.org/10.1007/s41060-017-0085-7">https://doi.org/10.1007/s41060-017-0085-7</a></p>
</dd>
</dl>
<dl class="py method">
<dt class="sig sig-object py" id="pgmpy.estimators.BICCondGauss.local_score">
<span class="sig-name descname"><span class="pre">local_score</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">variable</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">parents</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/pgmpy/estimators/StructureScore.html#BICCondGauss.local_score"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#pgmpy.estimators.BICCondGauss.local_score" title="Link to this definition">¶</a></dt>
<dd><p>Computes the local BIC score for a variable and its parent set in a mixed Bayesian network.</p>
<p>The score is calculated as the log-likelihood minus a complexity penalty, which
is proportional to the number of free parameters and the log of the sample size.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters<span class="colon">:</span></dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>variable</strong> (<em>str</em>) – The name of the variable (node) for which the local score is to be computed.</p></li>
<li><p><strong>parents</strong> (<em>list</em><em> of </em><em>str</em>) – List of variable names considered as parents of <cite>variable</cite>.</p></li>
</ul>
</dd>
<dt class="field-even">Returns<span class="colon">:</span></dt>
<dd class="field-even"><p><strong>score</strong> – The local BICCondGauss score for the specified variable and parent configuration.</p>
</dd>
<dt class="field-odd">Return type<span class="colon">:</span></dt>
<dd class="field-odd"><p>float</p>
</dd>
</dl>
<p class="rubric">Examples</p>
<div class="doctest highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">s</span> <span class="o">=</span> <span class="n">score</span><span class="o">.</span><span class="n">local_score</span><span class="p">(</span><span class="s2">&quot;A&quot;</span><span class="p">,</span> <span class="p">[</span><span class="s2">&quot;B&quot;</span><span class="p">,</span> <span class="s2">&quot;C&quot;</span><span class="p">])</span>
<span class="gp">&gt;&gt;&gt; </span><span class="nb">print</span><span class="p">(</span><span class="n">s</span><span class="p">)</span>
<span class="go">-115.37</span>
</pre></div>
</div>
<dl class="field-list simple">
<dt class="field-odd">Raises<span class="colon">:</span></dt>
<dd class="field-odd"><p><strong>ValueError</strong> – If the log-likelihood or parameter count cannot be computed for the given configuration.</p>
</dd>
</dl>
</dd></dl>

</dd></dl>

</section>
<section id="conditional-gaussian-aic-score">
<h2>Conditional Gaussian AIC Score<a class="headerlink" href="#conditional-gaussian-aic-score" title="Link to this heading">¶</a></h2>
<dl class="py class">
<dt class="sig sig-object py" id="pgmpy.estimators.AICCondGauss">
<em class="property"><span class="k"><span class="pre">class</span></span><span class="w"> </span></em><span class="sig-prename descclassname"><span class="pre">pgmpy.estimators.</span></span><span class="sig-name descname"><span class="pre">AICCondGauss</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">data</span></span></em>, <em class="sig-param"><span class="o"><span class="pre">**</span></span><span class="n"><span class="pre">kwargs</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/pgmpy/estimators/StructureScore.html#AICCondGauss"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#pgmpy.estimators.AICCondGauss" title="Link to this definition">¶</a></dt>
<dd><p>AIC (Akaike Information Criterion) score for Bayesian networks with mixed (discrete and continuous) variables.</p>
<p>The AICCondGauss score evaluates network structures by penalizing the conditional log-likelihood
with a term equal to the number of free parameters. This generalizes the classic AIC
to handle Bayesian networks with both discrete and continuous variables [1].</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters<span class="colon">:</span></dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>data</strong> (<em>pandas.DataFrame</em>) – DataFrame where columns may be discrete or continuous variables.</p></li>
<li><p><strong>state_names</strong> (<em>dict</em><em>, </em><em>optional</em>) – Dictionary mapping discrete variable names to possible states.</p></li>
</ul>
</dd>
</dl>
<p class="rubric">Examples</p>
<div class="doctest highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="kn">import</span> <span class="nn">pandas</span> <span class="k">as</span> <span class="nn">pd</span>
<span class="gp">&gt;&gt;&gt; </span><span class="kn">import</span> <span class="nn">numpy</span> <span class="k">as</span> <span class="nn">np</span>
<span class="gp">&gt;&gt;&gt; </span><span class="kn">from</span> <span class="nn">pgmpy.estimators</span> <span class="kn">import</span> <span class="n">AICCondGauss</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">data</span> <span class="o">=</span> <span class="n">pd</span><span class="o">.</span><span class="n">DataFrame</span><span class="p">(</span>
<span class="gp">... </span>    <span class="p">{</span>
<span class="gp">... </span>        <span class="s2">&quot;A&quot;</span><span class="p">:</span> <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="mi">100</span><span class="p">),</span>
<span class="gp">... </span>        <span class="s2">&quot;B&quot;</span><span class="p">:</span> <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">randint</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">100</span><span class="p">),</span>
<span class="gp">... </span>        <span class="s2">&quot;C&quot;</span><span class="p">:</span> <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="mi">100</span><span class="p">),</span>
<span class="gp">... </span>    <span class="p">}</span>
<span class="gp">... </span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">score</span> <span class="o">=</span> <span class="n">AICCondGauss</span><span class="p">(</span><span class="n">data</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">s</span> <span class="o">=</span> <span class="n">score</span><span class="o">.</span><span class="n">local_score</span><span class="p">(</span><span class="s2">&quot;A&quot;</span><span class="p">,</span> <span class="p">[</span><span class="s2">&quot;B&quot;</span><span class="p">,</span> <span class="s2">&quot;C&quot;</span><span class="p">])</span>
<span class="gp">&gt;&gt;&gt; </span><span class="nb">print</span><span class="p">(</span><span class="n">s</span><span class="p">)</span>
<span class="go">-99.75</span>
</pre></div>
</div>
<dl class="field-list simple">
<dt class="field-odd">Raises<span class="colon">:</span></dt>
<dd class="field-odd"><p><strong>ValueError</strong> – If the log-likelihood or number of parameters cannot be computed for the provided variables.</p>
</dd>
</dl>
<p class="rubric">References</p>
<dl class="simple">
<dt>[1] Andrews, B., Ramsey, J., &amp; Cooper, G. F. (2018). Scoring Bayesian</dt><dd><p>Networks of Mixed Variables. International journal of data science and
analytics, 6(1), 3–18. <a class="reference external" href="https://doi.org/10.1007/s41060-017-0085-7">https://doi.org/10.1007/s41060-017-0085-7</a></p>
</dd>
</dl>
<dl class="py method">
<dt class="sig sig-object py" id="pgmpy.estimators.AICCondGauss.local_score">
<span class="sig-name descname"><span class="pre">local_score</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">variable</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">parents</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/pgmpy/estimators/StructureScore.html#AICCondGauss.local_score"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#pgmpy.estimators.AICCondGauss.local_score" title="Link to this definition">¶</a></dt>
<dd><p>Computes the local AIC score for a variable and its parent set in a mixed Bayesian network.</p>
<p>The score is calculated as the log-likelihood minus the number of free parameters.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters<span class="colon">:</span></dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>variable</strong> (<em>str</em>) – The name of the variable (node) for which the local score is to be computed.</p></li>
<li><p><strong>parents</strong> (<em>list</em><em> of </em><em>str</em>) – List of variable names considered as parents of <cite>variable</cite>.</p></li>
</ul>
</dd>
<dt class="field-even">Returns<span class="colon">:</span></dt>
<dd class="field-even"><p><strong>score</strong> – The local AICCondGauss score for the specified variable and parent configuration.</p>
</dd>
<dt class="field-odd">Return type<span class="colon">:</span></dt>
<dd class="field-odd"><p>float</p>
</dd>
</dl>
<p class="rubric">Examples</p>
<div class="doctest highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">s</span> <span class="o">=</span> <span class="n">score</span><span class="o">.</span><span class="n">local_score</span><span class="p">(</span><span class="s2">&quot;A&quot;</span><span class="p">,</span> <span class="p">[</span><span class="s2">&quot;B&quot;</span><span class="p">,</span> <span class="s2">&quot;C&quot;</span><span class="p">])</span>
<span class="gp">&gt;&gt;&gt; </span><span class="nb">print</span><span class="p">(</span><span class="n">s</span><span class="p">)</span>
<span class="go">-99.75</span>
</pre></div>
</div>
<dl class="field-list simple">
<dt class="field-odd">Raises<span class="colon">:</span></dt>
<dd class="field-odd"><p><strong>ValueError</strong> – If the log-likelihood or parameter count cannot be computed for the given configuration.</p>
</dd>
</dl>
</dd></dl>

</dd></dl>

</section>
</section>


          </div>
          
        </div>
      </div>
      <div class="sphinxsidebar" role="navigation" aria-label="Main">
        <div class="sphinxsidebarwrapper">
<p class="logo">
  <a href="../index.html">
    <img class="logo" src="../_static/logo.png" alt="Logo" />
    
  </a>
</p>









<search id="searchbox" style="display: none" role="search">
  <h3 id="searchlabel">Quick search</h3>
    <div class="searchformwrapper">
    <form class="search" action="../search.html" method="get">
      <input type="text" name="q" aria-labelledby="searchlabel" autocomplete="off" autocorrect="off" autocapitalize="off" spellcheck="false"/>
      <input type="submit" value="Go" />
    </form>
    </div>
</search>
<script>document.getElementById('searchbox').style.display = "block"</script><h3>Navigation</h3>
<ul class="current">
<li class="toctree-l1"><a class="reference internal" href="../started/base.html">Getting Started</a></li>
<li class="toctree-l1"><a class="reference internal" href="../examples.html">Examples</a></li>
<li class="toctree-l1"><a class="reference internal" href="../models/base.html">Supported Models</a></li>
<li class="toctree-l1"><a class="reference internal" href="../factors/base.html">Parameterization</a></li>
<li class="toctree-l1"><a class="reference internal" href="../infer/base.html">Probabilistic Inference</a></li>
<li class="toctree-l1"><a class="reference internal" href="../causal_infer/base.html">Causal Inference</a></li>
<li class="toctree-l1"><a class="reference internal" href="../param_estimator/base.html">Parameter Estimation</a></li>
<li class="toctree-l1 current"><a class="reference internal" href="base.html">Causal Discovery / Structure Learning</a><ul class="current">
<li class="toctree-l2"><a class="reference internal" href="pc.html">PC (Constraint-Based Estimator)</a></li>
<li class="toctree-l2"><a class="reference internal" href="pc.html#module-pgmpy.estimators.CITests">Conditional Independence Tests for PC algorithm</a></li>
<li class="toctree-l2 current"><a class="current reference internal" href="#">Hill Climb Search</a></li>
<li class="toctree-l2"><a class="reference internal" href="#structure-scores">Structure Scores</a></li>
<li class="toctree-l2"><a class="reference internal" href="ges.html">Greedy Equivalence Search (GES)</a></li>
<li class="toctree-l2"><a class="reference internal" href="tree.html">Tree Search</a></li>
<li class="toctree-l2"><a class="reference internal" href="expert.html">Expert In The Loop</a></li>
<li class="toctree-l2"><a class="reference internal" href="mmhc.html">Mmhc Estimator</a></li>
<li class="toctree-l2"><a class="reference internal" href="exhaustive.html">Exhaustive Search</a></li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="../metrics/metrics.html">Metrics for Testing Models</a></li>
<li class="toctree-l1"><a class="reference internal" href="../readwrite/base.html">Reading/Writing to File</a></li>
<li class="toctree-l1"><a class="reference internal" href="../plotting.html">Plotting Models</a></li>
<li class="toctree-l1"><a class="reference internal" href="../tutorial.html">Tutorial Notebooks</a></li>
</ul>

<div class="relations">
<h3>Related Topics</h3>
<ul>
  <li><a href="../index.html">Documentation overview</a><ul>
  <li><a href="base.html">Causal Discovery / Structure Learning</a><ul>
      <li>Previous: <a href="pc.html" title="previous chapter">PC (Constraint-Based Estimator)</a></li>
      <li>Next: <a href="ges.html" title="next chapter">Greedy Equivalence Search (GES)</a></li>
  </ul></li>
  </ul></li>
</ul>
</div><script async src="https://media.ethicalads.io/media/client/ethicalads.min.js"></script>

<div data-ea-publisher="pgmpyorg" data-ea-type="image" data-ea-style="horizontal"></div><script async src="https://www.googletagmanager.com/gtag/js?id=G-HCFR07M31W"></script>
<script>
  window.dataLayer = window.dataLayer || [];
  function gtag(){dataLayer.push(arguments);}
  gtag('js', new Date());

  gtag('config', 'G-HCFR07M31W');
</script>
        </div>
      </div>
      <div class="clearer"></div>
    </div>
    <div class="footer">
      &#169;2025, Ankur Ankan.
      
      |
      Powered by <a href="https://www.sphinx-doc.org/">Sphinx 8.2.3</a>
      &amp; <a href="https://alabaster.readthedocs.io">Alabaster 0.7.16</a>
      
      |
      <a href="../_sources/structure_estimator/hill.rst.txt"
          rel="nofollow">Page source</a>
    </div>

    

    
  </body>
</html>